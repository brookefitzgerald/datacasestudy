---
title: "Housing Data Analysis"
author: "Brooke Fitzgerald, Gloria Giramahoro, Sergi Drago"
date: "4/2/2017"
output: html_document
---

# Introduction
This data analysis project set out to answer two questions: 
1. Which variables are most important to determining the sale price of a house?
2. Are there natural clusters of houses that can be derived and explained from within the data and what do those clusters correspond to?

The dataset we set out to analyze was found on the Kaggle Competition: “House Prices: Advanced Regression Techniques.” The original data comes from the Ames City Assessor’s Office in the form of a data dump from their records system. The initial Excel file contained 113 variables describing 3970 property sales that had occurred in this city between 2006 and 2010. This data was obtained by Prof. De Cock (from Truman State University) who removed variables that required special knowledge or previous calculations for their use, leaving us with the 80 variable dataset we have now.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(scales)
library(ggplot2)
library(GGally)
library(ggthemes)
library(MASS)
library(dplyr)
library(Rtsne)
library(amap)
library(dummies)
library(rpart)
library(caret)
library(gam)
library(gbm)
library(mice)
library(ipred)
library(kknn)
library(randomForest)
library(Metrics)
library(caret)
library(graphics)
library(rattle)
library(gridExtra)
library(leaps)
library(glmnet)
set.seed(3000)
```

## Loading and cleaning the data

The first step in the data analysis process is data cleaning. This dataset needed quite a bit of cleaning. Many of the variables were only valid in some situations, like PoolQC - a variable that encodes the quality of a pool. In the data descriptions, it stated that if a house didn't have a pool, the variable was encoded as NA. This also artificially inflated the number of rows that were missing values when in fact the NA was simply coding for the lack of an attribute (pool, garage, deck, etc.). 

Therefore, we decided that for all of the variables that used NA to encode a missing attribute, we replaced the NA values for a factor "None" to explicitely encode for that missing attribute. This brought the number of complete observations up from 0 to 1451 out of 1460.

```{r importdata}
fullTrain <- read.csv("train.csv")
fullTest <- read.csv("test.csv")
paste("Initial complete observations: ", dim(fullTrain[complete.cases(fullTrain),])[1])

fullTrain$LotFrontage[is.na(fullTrain$LotFrontage)]<-0.0

levels(fullTrain$Alley)[length(levels(fullTrain$Alley))+1] = "None"
fullTrain$Alley[is.na(fullTrain$Alley)]<-"None"

levels(fullTrain$PoolQC)[length(levels(fullTrain$PoolQC))+1] = "None"
fullTrain$PoolQC[is.na(fullTrain$PoolQC)]<-"None"

levels(fullTrain$Fence)[length(levels(fullTrain$Fence))+1] = "None"
fullTrain$Fence[is.na(fullTrain$Fence)]<-"None"

levels(fullTrain$MiscFeature)[length(levels(fullTrain$MiscFeature))+1] = "None"
fullTrain$MiscFeature[is.na(fullTrain$MiscFeature)]<-"None"

levels(fullTrain$GarageQual)[length(levels(fullTrain$GarageQual))+1] = "None"
fullTrain$GarageQual[is.na(fullTrain$GarageQual)]<-"None"

levels(fullTrain$GarageCond)[length(levels(fullTrain$GarageCond))+1] = "None"
fullTrain$GarageCond[is.na(fullTrain$GarageCond)]<-"None"

levels(fullTrain$BsmtQual)[length(levels(fullTrain$BsmtQual))+1] = "None"
fullTrain$BsmtQual[is.na(fullTrain$BsmtQual)]<-"None"

levels(fullTrain$BsmtCond)[length(levels(fullTrain$BsmtCond))+1] = "None"
fullTrain$BsmtCond[is.na(fullTrain$BsmtCond)]<-"None"

levels(fullTrain$BsmtExposure)[length(levels(fullTrain$BsmtExposure))+1] = "None"
fullTrain$BsmtExposure[is.na(fullTrain$BsmtExposure)]<-"None"

levels(fullTrain$BsmtFinType1)[length(levels(fullTrain$BsmtFinType1))+1] = "None"
fullTrain$BsmtFinType1[is.na(fullTrain$BsmtFinType1)]<-"None"

levels(fullTrain$BsmtFinType2)[length(levels(fullTrain$BsmtFinType2))+1] = "None"
fullTrain$BsmtFinType2[is.na(fullTrain$BsmtFinType2)]<-"None"

levels(fullTrain$FireplaceQu)[length(levels(fullTrain$FireplaceQu))+1] = "None"
fullTrain$FireplaceQu[is.na(fullTrain$FireplaceQu)]<-"None"

levels(fullTrain$GarageYrBlt)[length(levels(fullTrain$GarageYrBlt))+1] = "None"
fullTrain$GarageYrBlt[is.na(fullTrain$GarageYrBlt)]<-"None"

levels(fullTrain$GarageType)[length(levels(fullTrain$GarageType))+1] = "None"
fullTrain$GarageType[is.na(fullTrain$GarageType)]<-"None"

levels(fullTrain$GarageFinish)[length(levels(fullTrain$GarageFinish))+1] = "None"
fullTrain$GarageFinish[is.na(fullTrain$GarageFinish)]<-"None"

paste("Complete observations coded for missing attributes: " , dim(fullTrain[complete.cases(fullTrain),])[1])
```

```{r,echo=FALSE}
fullTest$LotFrontage[is.na(fullTest$LotFrontage)]<-0.0

levels(fullTest$Alley)[length(levels(fullTest$Alley))+1] = "None"
fullTest$Alley[is.na(fullTest$Alley)]<-"None"

levels(fullTest$PoolQC)[length(levels(fullTest$PoolQC))+1] = "None"
fullTest$PoolQC[is.na(fullTest$PoolQC)]<-"None"

levels(fullTest$Fence)[length(levels(fullTest$Fence))+1] = "None"
fullTest$Fence[is.na(fullTest$Fence)]<-"None"

levels(fullTest$MiscFeature)[length(levels(fullTest$MiscFeature))+1] = "None"
fullTest$MiscFeature[is.na(fullTest$MiscFeature)]<-"None"

levels(fullTest$GarageQual)[length(levels(fullTest$GarageQual))+1] = "None"
fullTest$GarageQual[is.na(fullTest$GarageQual)]<-"None"

levels(fullTest$GarageCond)[length(levels(fullTest$GarageCond))+1] = "None"
fullTest$GarageCond[is.na(fullTest$GarageCond)]<-"None"

levels(fullTest$BsmtQual)[length(levels(fullTest$BsmtQual))+1] = "None"
fullTest$BsmtQual[is.na(fullTest$BsmtQual)]<-"None"

levels(fullTest$BsmtCond)[length(levels(fullTest$BsmtCond))+1] = "None"
fullTest$BsmtCond[is.na(fullTest$BsmtCond)]<-"None"

levels(fullTest$BsmtExposure)[length(levels(fullTest$BsmtExposure))+1] = "None"
fullTest$BsmtExposure[is.na(fullTest$BsmtExposure)]<-"None"

levels(fullTest$BsmtFinType1)[length(levels(fullTest$BsmtFinType1))+1] = "None"
fullTest$BsmtFinType1[is.na(fullTest$BsmtFinType1)]<-"None"

levels(fullTest$BsmtFinType2)[length(levels(fullTest$BsmtFinType2))+1] = "None"
fullTest$BsmtFinType2[is.na(fullTest$BsmtFinType2)]<-"None"

levels(fullTest$FireplaceQu)[length(levels(fullTest$FireplaceQu))+1] = "None"
fullTest$FireplaceQu[is.na(fullTest$FireplaceQu)]<-"None"

levels(fullTest$GarageType)[length(levels(fullTest$GarageType))+1] = "None"
fullTest$GarageType[is.na(fullTest$GarageType)]<-"None"

levels(fullTest$GarageFinish)[length(levels(fullTest$GarageFinish))+1] = "None"
fullTest$GarageFinish[is.na(fullTest$GarageFinish)]<-"None"

fullTest<-dplyr::select(fullTest,-GarageYrBlt)
fullTrain<-dplyr::select(fullTrain,-GarageYrBlt)
```

## Exploratory Data Analysis

A summary of the data shows that many variables have levels with only a few observations in them. For example, the RoofMatl variable (which describes the material of the roof) has 8 different options, clay tile, standard composite shingle, membrane, metal, roll, gravel and tar, wood shakes, and wood shingles. However, out of all 1460 observations, clay tile, membrane, metal, and roll roofs only have one observation, wood shake has 5 observations, and wood shingle has 6.

A similar pattern is true for many other categorical variables, where certain categories are very sparsely represented. This is potentially a problem for building cross validated models due to the fact that a model that is trained on a subset of the data is unlikely to see many examples of all categories and thus might be unable to accurately model the relationships between the categories and sale price. 
 
```{r summarize}
summary(fullTrain)
```

Then with some data visualization we find that the sale price of homes has a positively skewed distribution, with the median sale price of a house (\$163,000.00) being significantly less than the average sale price of a house (\$180,921.20).
```{r visualization}
options(scipen=5)

ggplot(fullTrain, aes(SalePrice))+
       geom_histogram(bins = 25)+
       geom_vline(data = data.frame(SalePriceSummary = c("Mean Sale Price", "Median Sale Price"), Summary = c(mean(fullTrain$SalePrice),median(fullTrain$SalePrice))),
                 aes(xintercept = Summary,
                 linetype=SalePriceSummary,
                 colour = SalePriceSummary),
                 show.legend = TRUE)+
       theme_few()
```

A boxplot comparing the sale prices for all of the different neighborhoods shows that there are some neighborhoods that clearly have much higher average and median sale prices than others. This indicates that Neighborhood is likely an importat indicator for sale price.
```{r neighborhoods}
ordered_names<-fullTrain %>% group_by(Neighborhood)%>%summarize(avg = mean('SalePrice')) %>% arrange(avg)
ordered_neighborhood_dfm <- data.frame(Neighborhood=factor(fullTrain$Neighborhood,
    levels = ordered_names$Neighborhood,ordered = TRUE),
    SalePrice = fullTrain$SalePrice)

ggplot(ordered_neighborhood_dfm, aes(x=Neighborhood,y=SalePrice))+
       geom_boxplot()+
       theme_few()+
       coord_flip()

```

We also wanted to see if which month a house was purchased in has any effect on the sale price of the house. Based on the plots of the number of sales per month, colored by the mean and median sale price for each month, it seems like house sale prices are lower and more frequent as the summer starts (April, May), but that buying a house in September is going to be much more expensive then during other months.
```{r month_sold}
p1<-fullTrain %>% group_by(MoSold) %>% summarize(NumSold=n(),MeanSalePrice=mean(SalePrice))%>%
  ggplot(aes(x=MoSold,y=NumSold, fill=MeanSalePrice))+
         geom_bar(stat='identity')+
         scale_x_continuous(breaks = 1:12)+
         theme_few()
p2<-fullTrain %>% group_by(MoSold) %>% summarize(NumSold=n(),MedianSalePrice=median(SalePrice))%>%
  ggplot(aes(x=MoSold,y=NumSold, fill=MedianSalePrice))+
         geom_bar(stat='identity')+
         scale_x_continuous(breaks = 1:12)+
         theme_few()
grid.arrange(p1,p2,nrow=2)
```

In the plots of sale price and the year built (YearBuilt), overall house quality (OverallQual), total number of rooms above ground (TotRmsAbvGrd), and above grade living area square feet (GrLivArea), it becomes clear that these three variables are positively correlated with sale price. 
```{r scatter_plots}
p1<-ggplot(fullTrain, aes(x=YearBuilt,y=SalePrice))+
       geom_point(alpha = 0.4)+
       theme_few()+
       scale_x_continuous(breaks = seq(1872,2020,20))
p2<-ggplot(fullTrain, aes(x=OverallQual,y=SalePrice))+
       geom_jitter(alpha = 0.4)+
       theme_few()+
       scale_x_continuous(breaks = seq(0,10,1))
p3<-ggplot(fullTrain, aes(x=TotRmsAbvGrd,y=SalePrice))+
       geom_jitter(alpha = 0.4)+
       theme_few()+
       scale_x_continuous(breaks = seq(0,14,1))
p4<-ggplot(fullTrain, aes(x = GrLivArea, y=SalePrice))+
       geom_point(alpha = 0.4)+
       theme_few()
grid.arrange(p1,p2,p3,p4,nrow=2)

```
Because year built, overall house quality, total number of rooms above ground, and above grade living area square feet seemed correlated with SalePrice, we wanted to definitively test said correlation and see if there are other variables that are highly correlated with SalePrice.

However, since correlation requires numeric variables, and since many of our variables are factors, we decided to use a technique known as one hot variable encoding. With this technique, each variable with $n$ factors is expanded into $n$ variables that take the value 1 if the data has that factor and 0 otherwise. 

For example, the variable Street with its two possible factors 'Gravl' and 'Pave' becomes two variables: StreetGravl and StreetPave that are either 0 or 1. 

With this new encoding we calculated the correlation and then plotted the variables with absolute correlation values larger than 0.5.
```{r correlation}
one_hot_encoded<- dummy.data.frame(fullTrain)
names(one_hot_encoded)<-make.names(names(one_hot_encoded))
one_hot_encoded_cc <- one_hot_encoded[complete.cases(one_hot_encoded),]

corprice<-cor(one_hot_encoded_cc)[,"SalePrice"]
corprice<-corprice[names(corprice)!="SalePrice"]
sub_corprice <- na.omit(corprice[abs(corprice)>0.5])

ggplot()+
  geom_bar(aes(x=reorder(names(sub_corprice), -sub_corprice),sub_corprice),stat="identity")+
  theme_few()+
  coord_flip()+
  labs(y="Correlation to SalePrice", x = "Variable")
```


## Removing NA values
Because some predictive models don't allow for NA values, we want to do some data imputation. The data was imputed using random forests. This imputed data can be used in conjunction with the one-hot encoded data to compare and contrast the various predictions.

```{r imputation, echo=F}
imp <- mice(fullTrain,method='rf')
imputed <- fullTrain

# This function takes in a row of categories, then returns the category occuring most often
f<- function(dataRow){
       t<-table(dataRow)
       return(names(sort(t,decreasing = TRUE)[1]))
}
i=1
for(varName in imp$imp){
       if (!is.null(varName[[1]])){
              if (is.numeric(varName[[1]])){
                    # if data is numeric, impute the mean 
                     imputed[row.names(varName),names(imputed)[i]] = round(rowMeans(varName))
              } else {
                    # if data is categorical, impute the most often predicted category
                     imputed[row.names(varName),names(imputed)[i]] = apply(varName, 1, f)
              }
              
       }
       i<-i+1
}

```
## Starting Predictive Modeling
The first step in predictive modeling is splitting the data into training and test sets.

We did this by using the createDataPartition function from the caret package in order to get training and testing sets that have values of SalePrice that are evenly split across the dataset.

We created training and testing sets for the full data, the imputed data, the one hot encoded data, and the one hot encoded data with NA values removed.

```{r partition_data}
indices<- createDataPartition(fullTrain$SalePrice,p=0.75)$Resample1
train.full<- fullTrain[indices,]
test.full<-fullTrain[-indices,]

train.imp<- imputed[indices,]
test.imp<-imputed[-indices,]

train.onehot<- one_hot_encoded[indices,]
test.onehot<-one_hot_encoded[-indices,]

train.onehot.cc <-train.onehot[complete.cases(train.onehot),]
test.onehot.cc <-test.onehot[complete.cases(test.onehot),]
```

The first model we tried out was a simple decision tree. We created decision trees on all of our training sets and calculated the cross-validated RMSE for each. We also averaged the predictions from each decision tree and calculated the cross-validated RMSE for that. Overall, the predictions from the decision trees weren't too inaccurate, though they tended to underpredict sale price for more expensive homes. Furthermore, the average of the predictions from all three trees has the lowest RMSE, suggesting that a random forest might increase the accuracy of the model.

We then decided to answer our second research question:  Which variables are most important to determining the sale price of a house?

To do this, we inspected the individual decision trees the algorithm constructed. Each algorithm constructed a decision tree that has 11 leaf nodes of sale price bins that are reached with either 3 or 4 splits. 

From these, it becomes immediately obvious that the overall quality of a house is the most important factor to sale price, as it was used for all three trees for the first and second level splits. The third level splits are equally in agreement, for all three models use above grade living area square feet to split this level. After that, the trees are not as synonomous, though they do contain similar information about the important variables in prediction sale price.

All three models use the size of garage in car capacity to determine the data with the 3rd and 4th lowest sale price nodes and the finished square feet of the basement type 1 to determine the 4th and 5th highest sale price nodes.

Where the decision trees diverge is the variable used to split between the 2 lowest sale price nodes. For the full data and the imputed data the input data point is assigned to the lowest node if it is in any of the neighborhoods Briardale, Brookside, Edwards, Iowa DOT and Rail Road, Meadow Village, Old Town, or South & West of Iowa State University. However, for the one-hot encoded data the input data point is assigned to the lowest sale price node if the total basement square feet is less than $1008{\ }ft^2$.

When you look at the box plots comparing the two splits, the neighborhood split has a smaller interquartile range within sale price than the total basement square feet split. This may explain why the one-hot encoded decision tree has a slightly worse RMSE than the other two models. 

```{r decision_tree}
tree.model.full <-rpart(SalePrice~.,train.full)
tree.preds.full <- predict(tree.model.full, test.full)
tree.RMSE.full <- sqrt(sum((tree.preds.full-test.full$SalePrice)^2))/length(tree.preds.full)
tree.RMSE.full
p1<-qplot(tree.preds.full,test.full$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = paste("Full Regression Tree RMSE: ",tree.RMSE.full),y="Sale Price ")

tree.model.onehot <-rpart(SalePrice~.,train.onehot)
tree.preds.onehot <- predict(tree.model.onehot, test.onehot)
tree.RMSE.onehot <- sqrt(sum((tree.preds.onehot-test.onehot$SalePrice)^2))/length(tree.preds.onehot)
tree.RMSE.onehot
p2<-qplot(tree.preds.onehot,test.onehot$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = paste("One-hot Regression Tree RMSE: ",tree.RMSE.onehot),y="Sale Price ")

tree.model.imp <- rpart(SalePrice~.,train.imp)
tree.preds.imp <- predict(tree.model.imp, test.imp)
tree.RMSE.imp <- sqrt(sum((tree.preds.imp-test.imp$SalePrice)^2))/length(tree.preds.imp)
tree.RMSE.imp
p3<-qplot(tree.preds.imp,test.imp$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = paste("Imputed Regression Tree RMSE: ",tree.RMSE.imp),y="Sale Price ")

tree.preds.avg <- rowMeans(cbind(tree.preds.full,tree.preds.onehot,tree.preds.imp))
tree.RMSE.avg <- sqrt(sum((tree.preds.avg-test.full$SalePrice)^2))/length(tree.preds.avg)
tree.RMSE.avg
p4<-qplot(tree.preds.avg,test.full$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = paste("Average Regression Tree RMSE: ",tree.RMSE.avg),y="Sale Price ")

grid.arrange(p1,p2,p3,p4,ncol=2,nrow=2)

fancyRpartPlot(tree.model.full)
fancyRpartPlot(tree.model.imp)
fancyRpartPlot(tree.model.onehot)


lowest_vars<-fullTrain %>% mutate(LowestNeighborhoods = factor(ifelse(Neighborhood %in% c("BrDale","BrkSide","Edwards","IDOTRR","MeadowV","OldTown","SWISU"), 1, 0))) %>%
  mutate(LowestTotalBsmtSF = factor(ifelse(TotalBsmtSF<1008,1,0))) %>%
  select(LowestNeighborhoods,LowestTotalBsmtSF,SalePrice)

p1<-ggplot(lowest_vars, aes(LowestTotalBsmtSF,SalePrice))+
  geom_boxplot()+
  theme_few()
p2<-ggplot(lowest_vars, aes(LowestNeighborhoods,SalePrice))+
  geom_boxplot()+
  theme_few()
grid.arrange(p1,p2,ncol=2)

```

The next method we used to attempt to answer our research question is linear regression. Due to some of the factor variables having extremely sparse categories (as discussed above) when the data was split into training and testing sets the testing set occasionally had factors that were missing from the training set. To overcome this, we manually added the factors that were missing, but this understandably caused some deficiencies in the model and its predictions.

The linear models fit on the full dataset and the imputed dataset entirely failed to predict sale price, likely due to the lack of factor representation. However, the one-hot encoded data will always represent every factor category as a variable, even if that variable contains only zeros in the training split. The predictions from the one hot encoded data fit the data rather well with an adjusted $R^2$ of 0.9157 and RMSE of 1540.92.

However, when examining the statistically significant coefficients of the model in order to answer our research question, it becomes clear that interpreting the outputs of a model with such sparsely represented factors gives nonsensical results - e.g. that houses with a roof material of clay tile are worth \$600,000 less than those without clay roofs and that being adjacent to a near positive off-site feature--park, greenbelt, etc. makes your house worth almost \$200,000 less. 

Therefore, we filtered the one-hot encoded data to only get the variables with more than 10 observations in the training set and trained a linear model on that. The RMSE is slightly higher than the previous model at 1540.92, and the adjusted $R^2$ is slightly lower at 0.8736, but the coefficients are much more interpretable. 

From this linear model, we can say that having a quick and significant rise from street grade to building has the most significant negative impact on the sale price of a house, while being from a nice neighborhood (Northridge, Northridge Heights, Stone Brook), having excellent kitchen quality, typical functionality, and a large number of full bathrooms and garage capacity in number in cars all have a large impact on sale price.  

```{r linear_model}
lr.model.full <- lm(SalePrice~.,train.full, na.action = na.omit)
lr.model.full$xlevels$RoofMatl <- union(lr.model.full$xlevels$RoofMatl, levels(test.full$RoofMatl))
lr.model.full$xlevels$Exterior1st <- union(lr.model.full$xlevels$Exterior1st, levels(test.full$Exterior1st))
lr.model.full$xlevels$Exterior2nd <- union(lr.model.full$xlevels$Exterior2nd, levels(test.full$Exterior2nd))
lr.model.full$xlevels$Condition2 <- union(lr.model.full$xlevels$Condition2, levels(test.full$Condition2))
lr.model.full$xlevels$ExterCond <- union(lr.model.full$xlevels$ExterCond, levels(test.full$ExterCond))
lr.model.full$xlevels$Electrical <- union(lr.model.full$xlevels$Electrical, levels(test.full$Electrical))
lr.model.full$xlevels$Neighborhood <- union(lr.model.full$xlevels$Neighborhood, levels(test.full$Neighborhood))
lr.model.full$xlevels$RoofStyle <- union(lr.model.full$xlevels$RoofStyle, levels(test.full$RoofStyle))
lr.model.full$xlevels$Functional <- union(lr.model.full$xlevels$Functional, levels(test.full$Functional))
lr.model.full$xlevels$SaleType <- union(lr.model.full$xlevels$SaleType, levels(test.full$SaleType))
lr.model.full$xlevels$HeatingQC <- union(lr.model.full$xlevels$HeatingQC, levels(test.full$HeatingQC))
lr.model.full$xlevels$GarageQual <- union(lr.model.full$xlevels$GarageQual, levels(test.full$GarageQual))
lr.model.full$xlevels$GarageCond <- union(lr.model.full$xlevels$GarageCond, levels(test.full$GarageCond))
lr.preds.full <- predict(lr.model.full,test.full)
lr.RMSE.full <- sqrt(sum((lr.preds.full-test.full$SalePrice)^2,na.rm=TRUE))/length(lr.preds.full)
qplot(lr.preds.full,test.full$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Full Linear Model Predictions",y="Full Sale Price",title=paste("RMSE: ",lr.RMSE.full))

lr.model.imp <- lm(SalePrice~.,train.imp)
lr.model.imp$xlevels$Condition2 <- union(lr.model.imp$xlevels$Condition2, levels(test.imp$Condition2))
lr.model.imp$xlevels$Exterior1st <- union(lr.model.imp$xlevels$Exterior1st, levels(test.imp$Exterior1st))
lr.model.imp$xlevels$Exterior2nd <- union(lr.model.imp$xlevels$Exterior2nd, levels(test.imp$Exterior2nd))
lr.model.imp$xlevels$RoofMatl <- union(lr.model.imp$xlevels$RoofMatl, levels(test.imp$RoofMatl))
lr.model.imp$xlevels$Functional <- union(lr.model.imp$xlevels$Functional, levels(test.imp$Functional))
lr.model.imp$xlevels$BsmtCond <- union(lr.model.imp$xlevels$BsmtCond, levels(test.imp$BsmtCond))
lr.model.imp$xlevels$Heating <- union(lr.model.imp$xlevels$Heating, levels(test.imp$Heating))
lr.model.imp$xlevels$Electrical <- union(lr.model.imp$xlevels$Electrical, levels(test.imp$Electrical))
lr.model.imp$xlevels$MiscFeature <- union(lr.model.imp$xlevels$MiscFeature, levels(test.imp$MiscFeature))
lr.model.imp$xlevels$ExterCond <- union(lr.model.imp$xlevels$ExterCond, levels(test.imp$ExterCond))
lr.model.imp$xlevels$Neighborhood <- union(lr.model.imp$xlevels$Neighborhood, levels(test.imp$Neighborhood))
lr.model.imp$xlevels$RoofStyle <- union(lr.model.imp$xlevels$RoofStyle, levels(test.imp$RoofStyle))
lr.model.imp$xlevels$HeatingQC <- union(lr.model.imp$xlevels$HeatingQC, levels(test.imp$HeatingQC))
lr.model.imp$xlevels$GarageQual <- union(lr.model.imp$xlevels$GarageQual, levels(test.imp$GarageQual))
lr.model.imp$xlevels$GarageCond <- union(lr.model.imp$xlevels$GarageCond, levels(test.imp$GarageCond))
lr.model.imp$xlevels$SaleType <- union(lr.model.imp$xlevels$SaleType, levels(test.imp$SaleType))
lr.preds.imp <- predict(lr.model.imp,test.imp)
lr.RMSE.imp <- sqrt(sum((lr.preds.imp-test.imp$SalePrice)^2,na.rm = TRUE))/length(lr.preds.imp)
qplot(lr.preds.imp,test.imp$SalePrice[!is.na(lr.preds.imp)])+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Imputed Linear Model Predictions",y="Imputed Sale Price",title=paste("RMSE: ",lr.RMSE.imp))

lr.model.onehot <- lm(SalePrice~.,train.onehot, na.action = na.omit)
lr.preds.onehot <- predict(lr.model.onehot,test.onehot)
lr.RMSE.onehot <- sqrt(sum((lr.preds.onehot-test.onehot$SalePrice)^2,na.rm=T))/length(lr.preds.onehot)
qplot(lr.preds.onehot,test.onehot$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "One-hot Linear Model Predictions",y="One-hot Sale Price ", title=paste('RMSE: ',lr.RMSE.onehot))

plot(lr.model.onehot$residuals)
significant_vars<-rownames(data.frame(summary(lr.model.onehot)$coef[summary(lr.model.onehot)$coef[,4] <= .01, 4]))
sig_coefficients<-lr.model.onehot$coefficients[significant_vars]

ggplot()+
  geom_bar(aes(x=reorder(names(sig_coefficients), -sig_coefficients),sig_coefficients),stat="identity")+
  theme_few()+
  labs(title = "Significant p<0.01 One-Hot Encoded Coefficients",y="Linear Model Coefficients", x = "Variable")+
  coord_flip()

p1<-ggplot(fullTrain, aes(x=RoofMatl, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  coord_flip()
p2<-ggplot(fullTrain, aes(x=Condition2, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  coord_flip()
p3<-ggplot(fullTrain, aes(x=RoofStyle, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  coord_flip()
p4<-ggplot(fullTrain, aes(x=PoolQC, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  coord_flip()

grid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)

lr.model.onehot.edit <- lm(SalePrice~.,train.onehot%>%select_if(function(col) sum(col,na.rm=T)>10)%>%select(-PoolQCNone), na.action = na.omit)
lr.preds.onehot.edit <- predict(lr.model.onehot.edit,test.onehot)
lr.RMSE.onehot.edit <- sqrt(sum((lr.preds.onehot.edit-test.onehot$SalePrice)^2,na.rm=T))/length(lr.preds.onehot.edit)
lr.RMSE.onehot.edit
qplot(lr.preds.onehot.edit,test.onehot$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "One-hot Linear Model Predictions",y="One-hot Sale Price ", title=paste('RMSE: ',lr.RMSE.onehot.edit))

plot(lr.model.onehot.edit$residuals)
significant_vars<-rownames(data.frame(summary(lr.model.onehot.edit)$coef[summary(lr.model.onehot.edit)$coef[,4] <= .01, 4]))
sig_coefficients<-lr.model.onehot.edit$coefficients[significant_vars]

ggplot()+
  geom_bar(aes(x=reorder(names(sig_coefficients), -sig_coefficients),sig_coefficients),stat="identity")+
  theme_few()+
  labs(title = "Significant p<0.01 One-Hot Encoded Non-Sparse Variable Coefficients",y="Linear Model Coefficients", x = "Variable")+
  coord_flip()

```

Forward and backward subset selection was also used to shed some insight on our first research question. Both forward and backward selection selected the house id, overall condition, and garage area as the top 1st, 2nd, and 5th of the top five predictive variables. 	Where they differ is that the forward subset selection seems to select variables concerning the basement of the house, while the backward subset selection seems to select variables relating to the square feet of the house and also the kitchen quality.

These selected variables should be taken with a grain of salt because the same non-sparse variable selection process was used as with the linear models, but forward and backward subset selection identify variables that weren't identified as important by any other method. Because there are so many variables, it is too computationally intensive to calculate the best subset selection, but it is likely that forward and backward subset selection with this many variables is identifying a non-optimal solution.

```{r subset_selection}

fullTrain %>% group_by(RoofMatl) %>% summarize(n_houses = n(), avg_sale_price = mean(SalePrice), med_sale_price = median(SalePrice))

forward_subset<-regsubsets(SalePrice~.,data = train.onehot%>%select_if(function(col) sum(col,na.rm=T)>10)%>%select(-PoolQCNone),method="forward")
forward<-names(one_hot_encoded_cc)[summary(forward_subset)$which[5,]]

backward_subset <-regsubsets(SalePrice~.,data = train.onehot%>%select_if(function(col) sum(col,na.rm=T)>10)%>%select(-PoolQCNone),method="backward")
backward<-names(one_hot_encoded_cc)[summary(backward_subset)$which[5,]]

data.frame("Variable Importance Rank"=1:6,
           "Forward Selected Variables" = names(train.onehot%>%select_if(function(col) sum(col,na.rm=T)>10)%>%select(-PoolQCNone))[summary(forward_subset)$which[5,]], 
           "Backward Selected Variables" = names(train.onehot%>%select_if(function(col) sum(col,na.rm=T)>10)%>%select(-PoolQCNone))[summary(backward_subset)$which[5,]])
```

<<<<<<< HEAD
Below we applied the Bagging regression of Salesprice: nbagg =25(bootstrap replications i.e samples to replicate.)Bagging regression also known as  (bootstrap aggregating) was introduced by Breiman (1 996a) to reduce the variance of a predictor( Buhlmann & Yu,2002) In our case applying the Bagging regression on the predictor sale price will reduce its variance and improve its mean squared error. after applying the bagging regression, the Out-of-bag estimate of root mean squared error is 36260.96, the mean is 180827and there have been 25 samples which were replicated


=======
Below we applied the KNN regression on the SalePrice. The regression is basically fitting the best line to predict the SalePrice between neighbouring houses. we assumed 4 values of k which are 3,5,19 and 50. Among the 4 values,3 had the least root mean square error(RMSE). 

We went ahead and drew a qplot to illustrate the pattern of k from 1 to 50 and its corresponding RMSE. According to the graph, 6 is the value of k with the least RMSE hence the most accurate in comparison to the others.Above 6,The RMSE gradually increase with the increasing value of k hence overfitting.
>>>>>>> origin/master
# KNN regression
```{r}
fullTrain_knn3<-knnreg(train.onehot.cc[-306],train.onehot.cc$SalePrice, k=3);
summary(fullTrain_knn3)
#plot(test.onehot.cc$SalePrice,predict(fullTrain_knn3,test.onehot.cc[-306]))
knn.3.preds<-predict(fullTrain_knn3,test.onehot.cc[-306])
knn.3.RMSE<-sqrt(sum((knn.3.preds-test.onehot.cc$SalePrice)^2))/length(test.onehot.cc)
knn.3.RMSE

#using k =5
fullTrain_knn5<-knnreg(train.onehot.cc[-306],train.onehot.cc$SalePrice, k=5);
summary(fullTrain_knn5)
knn.5.preds<-predict(fullTrain_knn5,test.onehot.cc[-306])
knn.5.RMSE<-sqrt(sum((knn.5.preds-test.onehot.cc$SalePrice)^2))/length(test.onehot.cc)
knn.5.RMSE
  
sd(test.onehot.cc$SalePrice,predict(fullTrain_knn5,test.onehot.cc[-306]))
print(predict(fullTrain_knn5,test.onehot.cc[-306]))
plot(test.onehot.cc$SalePrice,predict(fullTrain_knn5,test.onehot.cc[-306]))

#using k =19
fullTrain_knn19<-knnreg(train.onehot.cc[-306],train.onehot.cc$SalePrice, k=20);
knn.19.preds <-predict(fullTrain_knn19,test.onehot.cc[-306])
knn.19.preds<-predict(fullTrain_knn19,test.onehot.cc[-306])
knn.19.RMSE<-sqrt(sum((knn.19.preds-test.onehot.cc$SalePrice)^2))/length(test.onehot.cc)
knn.19.RMSE

summary(test.onehot.cc$SalePrice,predict(fullTrain_knn19,test.onehot.cc[-306]))
sd(test.onehot.cc$SalePrice,predict(fullTrain_knn19,test.onehot.cc[-306]))
plot(test.onehot.cc$SalePrice,predict(fullTrain_knn19,test.onehot.cc[-306]))

#using k =50
fullTrain_knn50<-knnreg(train.onehot.cc[-306],train.onehot.cc$SalePrice, k=50);
knn.50.preds <-predict(fullTrain_knn50,test.onehot.cc[-306])
knn.50.preds<-predict(fullTrain_knn50,test.onehot.cc[-306])
knn.50.RMSE<-sqrt(sum((knn.50.preds-test.onehot.cc$SalePrice)^2))/length(test.onehot.cc)
knn.50.RMSE

p1<-qplot(test.onehot.cc$SalePrice,knn.3.preds)+theme_few()+geom_abline(slope = 1, intercept = 0) + labs(x="Actual Sale Price",y="Predictions, k=3",title=paste("RMSE: ",knn.3.RMSE))
p2<-qplot(test.onehot.cc$SalePrice,knn.5.preds)+theme_few()+geom_abline(slope = 1, intercept = 0) + labs(x="Actual Sale Price",y="Predictions, k=5",title=paste("RMSE: ",knn.5.RMSE))
p3<-qplot(test.onehot.cc$SalePrice,knn.19.preds)+theme_few()+geom_abline(slope = 1, intercept = 0) + labs(x="Actual Sale Price",y="Predictions, k=19",title=paste("RMSE: ",knn.19.RMSE))
p4<-qplot(test.onehot.cc$SalePrice,knn.50.preds)+theme_few()+geom_abline(slope = 1, intercept = 0) + labs(x="Actual Sale Price",y="Predictions, k=50",title=paste("RMSE: ",knn.50.RMSE))
grid.arrange(p1,p2,p3,p4,ncol=2)

RMSES <- numeric(50)
for (i in 1:50){
  knn.model<-knnreg(train.onehot.cc[-306],train.onehot.cc$SalePrice, k=i);
  knn.preds <-predict(knn.model,test.onehot.cc[-306])
  knn.preds<-predict(knn.model,test.onehot.cc[-306])
  RMSES[i]<-sqrt(sum((knn.preds-test.onehot.cc$SalePrice)^2))/length(test.onehot.cc)
}
qplot(1:50,RMSES)+theme_few()+labs(title = paste("Minimum RMSE at: ",which.min(RMSES)),x="k",y="RMSE")
```

We applied hierarchical clustering in order to classify our data into 5 clusters. This was to answer our second research questions "Are there intrinsic groupings/clusters of houses present in the data and what do these clusters represent?

"we classified the clusters in 5 groups. The 5 groups represent a range of sale prices in which the dataset fall. The 4th group being the most expensive costing within the range between 300000(3e+05) to 400000(4e+05) with outliers that go beyond 600000(6e+05) while the cheapest group  is the 5th group ranging from 100000(1e+05) but less than 200000(2e+05). Moreover we checked the overall quality compared to the price and the 4th group having the highest quality ranging within 7 up to 10 while the 5th group had the lowest quality ranging within 2 up to 6 with a few outliers which have 7.

Moreover, we checked as well the relationship between GrLivArea(Above grade (ground) living area square feet) and the sales price , we found that the 4th group's Gridliv area ranged between 1500 to 3500 with some outliers reaching to 4000 while the 5th group ranged between 500 to 2500. Interestingly,the second group's Gridliv area ranges between 1000 till almost 2000 however there are outliers that go beyond 5000.
```{r}
par(mar=rep(1, 4))
d <- dist(one_hot_encoded_cc, method = "euclidean") # distance matrix
hclust.fit <- hclust(d, method = "ward.D")
plot(hclust.fit) # display dendogram
hclust.groups <- cutree(hclust.fit, k = 5)
h_clust_data <- one_hot_encoded_cc %>% mutate(h_clust = hclust.groups)
h_clust_data$h_clust <- factor(h_clust_data$h_clust)

ggplot(h_clust_data, aes(h_clust,SalePrice))+
  geom_boxplot()+
  theme_few()

ggplot(h_clust_data, aes(GrLivArea,SalePrice, col=h_clust))+
  geom_point()+
  theme_few()

```
References
Peter Buhlmann & Bin Yu,2002,Analyzing Bagging


#Random Forests


```{r}
#5-fold cross validation
set.seed(100)
fullTrain<-fullTrain[complete.cases(fullTrain),]

folds = createFolds(fullTrain$SalePrice,k = 5)
train1 = fullTrain[-folds[[1]],1:80]
train2 = fullTrain[-folds[[2]],1:80]
train3 = fullTrain[-folds[[3]],1:80]
train4 = fullTrain[-folds[[4]],1:80]
train5 = fullTrain[-folds[[5]],1:80]

#cross validation "test" set (including the target value "SalePrice")
cvTest1 = fullTrain[folds[[1]],]
cvTest2 = fullTrain[folds[[2]],]
cvTest3 = fullTrain[folds[[3]],]
cvTest4 = fullTrain[folds[[4]],]
cvTest5 = fullTrain[folds[[5]],]

#Build 5 models
modelRF1 = randomForest(SalePrice ~ ., data = train1)
modelRF2 = randomForest(SalePrice ~ ., data = train2)
modelRF3 = randomForest(SalePrice ~ ., data = train3)
modelRF4 = randomForest(SalePrice ~ ., data = train4)
modelRF5 = randomForest(SalePrice ~ ., data = train5)

#Make predictions using cross validation "test" sets
Pred1 = predict(modelRF1, cvTest1)
Pred2 = predict(modelRF2, cvTest2)
Pred3 = predict(modelRF3, cvTest3)
Pred4 = predict(modelRF4, cvTest4)
Pred5 = predict(modelRF5, cvTest5)

#Calculate residuals
Res1 = cvTest1$SalePrice - Pred1
Res2 = cvTest2$SalePrice - Pred2
Res3 = cvTest3$SalePrice - Pred3
Res4 = cvTest4$SalePrice - Pred4
Res5 = cvTest5$SalePrice - Pred5

#Calculate RMSE
rmse1 = round(sqrt(sum((Res1)^2))/length(Pred1),2)
rmse2 = round(sqrt(sum((Res2)^2))/length(Pred2),2)
rmse3 = round(sqrt(sum((Res3)^2))/length(Pred3),2)
rmse4 = round(sqrt(sum((Res4)^2))/length(Pred4),2)
rmse5 = round(sqrt(sum((Res5)^2))/length(Pred5),2)
cbind(rmse1,rmse2,rmse3,rmse4,rmse5)


#Plot target vs. predicted for the 5 models
tp1 = ggplot(data = as.data.frame(cbind("Target" = cvTest1$SalePrice,"Predicted" = Pred1)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + labs(title= paste("RMSE: ",rmse1))
tp2 = ggplot(data = as.data.frame(cbind("Target" = cvTest2$SalePrice,"Predicted" = Pred2)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + labs(title= paste("RMSE: ",rmse2))
tp3 = ggplot(data = as.data.frame(cbind("Target" = cvTest3$SalePrice,"Predicted" = Pred3)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + labs(title= paste("RMSE: ",rmse3))
tp4 = ggplot(data = as.data.frame(cbind("Target" = cvTest4$SalePrice,"Predicted" = Pred4)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + labs(title=paste("RMSE: ", rmse4))
tp5 = ggplot(data = as.data.frame(cbind("Target" = cvTest5$SalePrice,"Predicted" = Pred5)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + labs(title=paste("RMSE: ",rmse5))
grid.arrange(tp1,tp2,tp3,tp4,tp5,ncol = 2)



```



```{r}

#feature importance
Imp1 = as.data.frame(modelRF1$importance)
Imp1$Predictor = rownames(Imp1)
Imp1 = Imp1[,c(2,1)]
rownames(Imp1) = c(1:nrow(Imp1))
colnames(Imp1)[2] = "Importance"
#ImpOrd1 = Imp1[order(Imp1$Importance, decreasing = TRUE),]

Imp2 = as.data.frame(modelRF2$importance)
Imp2$Predictor = rownames(Imp2)
Imp2 = Imp2[,c(2,1)]
rownames(Imp2) = c(1:nrow(Imp2))
colnames(Imp2)[2] = "Importance"
#ImpOrd2 = Imp2[order(Imp2$Importance, decreasing = TRUE),]

Imp3 = as.data.frame(modelRF3$importance)
Imp3$Predictor = rownames(Imp3)
Imp3 = Imp3[,c(2,1)]
rownames(Imp3) = c(1:nrow(Imp3))
colnames(Imp3)[2] = "Importance"
#ImpOrd3 = Imp3[order(Imp3$Importance, decreasing = TRUE),]

Imp4 = as.data.frame(modelRF4$importance)
Imp4$Predictor = rownames(Imp4)
Imp4 = Imp4[,c(2,1)]
rownames(Imp4) = c(1:nrow(Imp4))
colnames(Imp4)[2] = "Importance"
#ImpOrd4 = Imp4[order(Imp4$Importance, decreasing = TRUE),]

Imp5 = as.data.frame(modelRF5$importance)
Imp5$Predictor = rownames(Imp5)
Imp5 = Imp5[,c(2,1)]
rownames(Imp5) = c(1:nrow(Imp5))
colnames(Imp5)[2] = "Importance"
#ImpOrd5 = Imp5[order(Imp5$Importance, decreasing = TRUE),]

importance=data.frame(Predictor =ImpOrd1[,1],RForest1=ImpOrd1[,2],RForest2=ImpOrd2[,2],RForest3=ImpOrd3[,2],RForest4=ImpOrd4[,2],RForest5=ImpOrd5[,2])

dfm<-reshape2::melt(importance,id ="Predictor")
min_val<-quantile(dfm$value)[4]
dfm%>% filter(value>min_val)%>%mutate(value=value/min_val/5)%>%
  ggplot(aes(reorder(Predictor,value),value,fill=variable))+
  geom_bar(stat="identity")+
  coord_flip()+
  theme_few()


```


#Boosting 
```{r}
model.gbm <- gbm(SalePrice ~., data = train.full, distribution = "laplace",
             shrinkage = 0.05,
             interaction.depth = 5,
             bag.fraction = 0.66,
             n.minobsinnode = 1,
             cv.folds = 100,
             keep.data = F,
             verbose = F,
             n.trees = 300)
predict.gbm <- predict(model.gbm, test.full, n.trees = 300)




RMSE.gbm = round(sqrt(sum((predict-test.full$SalePrice)^2))/length(predict.gbm),2)  
plot.gbm <- predict-test.full$SalePrice


plot_gbm = ggplot(data = as.data.frame(cbind("Target" =test.full$SalePrice,"Predicted" = predict)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + labs(title= paste("RMSE: ",RMSE.gbm))

plot_gbm
```


```{r}
#feature importance
Importance.gbm = as.data.frame(summary(model.gbm))
Importance.gbm %>% filter(rel.inf>2) %>%
ggplot(aes(reorder(var,rel.inf),rel.inf))+
  geom_bar(stat="identity")+
  coord_flip()+
  theme_few()


```



Then we started looking into answering our second research question: Are there natural clusters of houses that can be derived and explained from within the data and what do those clusters correspond to? 

# Gloria's answers to research question 2 hierarchical clustering

# Sergi's answers to research question 2 knn 


Our first approach to answering this question was to do some dimensionality reduction to see if there were any obvious structures in the data.

First, we computed PCA on the data with two different algorithms, then plotted them. 

Sale price seems to almost exactly maps to the first principle component, and the second principle component is highly negatively correlated with LotArea. It is therefore obvious that much of the variation within this dataset is explained by these two variables. 

Furthermore, MSSubClass (the type of dwelling involved in the sale) also has an interesting relationship with principal component 2. The MSSubClasses 120, 160, and 180 form a cluster in the higher values of the second principal component. MSSubClasses 120, 160, and 180 are the 3 types of dwellings that are planned unit developments - communities of homes that are operated by a homeowners association to provide amenities like parks, playgrounds, pools, tennis and basketball courts, hiking trails, private gated common land and street lights or the like.

```{r pca}
numeric_columns_only <- fullTrain %>% select_if(is.numeric)
numeric_full_columns_only <- numeric_columns_only[complete.cases(numeric_columns_only),]

x<-princomp(as.matrix(numeric_full_columns_only))

numeric_col_w_prcmp <- numeric_full_columns_only %>% mutate(princomp1 = x$scores[,1], princomp2 = x$scores[,2])

cols_w_prcmp <- inner_join(numeric_col_w_prcmp, fullTrain)

outlier <- cols_w_prcmp %>% filter(princomp2<(-10000))

cols_w_prcmp <- cols_w_prcmp %>% filter(princomp2>(-10000))

p1<-ggplot(cols_w_prcmp, aes(x=princomp1, y=princomp2, col=log(SalePrice)))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
p2<-ggplot(cols_w_prcmp, aes(x=princomp1, y=princomp2, col=LotArea))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
p3<-qplot(princomp1,SalePrice,data = cols_w_prcmp)+theme_few()
p4<-qplot(princomp2,LotArea,data = cols_w_prcmp)+theme_few()
grid.arrange(p1,p2,p3,p4,ncol=2)

ggplot(cols_w_prcmp, aes(x=princomp1, y=princomp2, col=factor(MSSubClass)))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
p1<-ggplot(aes(factor(MSSubClass), princomp2), data=cols_w_prcmp)+
       geom_boxplot()+
       theme_few()+
       labs(x="MSSubClass")+
       coord_flip()
p2<-ggplot(fullTrain, aes(factor(MSSubClass), SalePrice))+
       geom_boxplot()+
       theme_few() +
       labs(x="MSSubClass")+
       coord_flip()

grid.arrange(p1,p2,ncol=2)

add_PUD <-function(MSSubClass){
  pud <- numeric(length(MSSubClass))
  for (i in 1:length(MSSubClass)){
    if (MSSubClass[i] %in% c(120, 160, 180)){
      pud[i] <- 1
    }
  }
  return(pud)
}
fullTrain <- fullTrain %>% mutate(pud = add_PUD(MSSubClass))
fullTest <- fullTest %>% mutate(pud = add_PUD(MSSubClass))

cols_w_prcmp <- cols_w_prcmp %>% mutate(pud = add_PUD(MSSubClass))

p1<-ggplot(aes(pud, princomp2, col=LotArea), data=cols_w_prcmp)+
       geom_jitter()+
       theme_few()+
       labs(x="PUD")
p2<-ggplot(cols_w_prcmp, aes(x=princomp1, y=princomp2, col=factor(pud)))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
grid.arrange(p1,p2)
```

Then we decided to compare dimensionality reduction of a t-SNE plot, which stands for t-distributed Stochastic Neighbor Embedding. It is a dimensionality reduction algorithm that embeds high dimensional data into a 2 dimensional space. There is a nice [R vignette about the package](https://cran.r-project.org/web/packages/tsne/tsne.pdf), a [blog post](https://www.codeproject.com/Tips/788739/Visualization-of-High-Dimensional-Data-using-t-SNE) about its use in R, and an [interactive visualization](http://distill.pub/2016/misread-tsne/) characterizing its strengths and weaknesses.

The t-SNE confirms many of the same things as the principal component analysis, namely that the majority of the variance in the dataset can be explained by the sale price (as well as variables that are correlated to the sale price like the overall quality). Also, when you plot variables that are uncorrelated with the SalePrice but correlated with the second principal component (like lot area and being a PUD) you see groupings on the outside of the main structure, and the occasional small cluster.

```{r tsne}
housing_tsne<-Rtsne(numeric_full_columns_only)

numeric_col_w_tsne<- numeric_full_columns_only %>% mutate(tsne1 = housing_tsne$Y[,1],tsne2 = housing_tsne$Y[,2])

cols_w_tsne <- inner_join(numeric_col_w_tsne, fullTrain)

ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=log(SalePrice)))+
       geom_point()+
       theme_few()
ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=factor(OverallQual)))+
       geom_point()+
       theme_few()
ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=log(LotArea)))+
       geom_point()+
       theme_few()

ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=factor(pud)))+
       geom_point()+
       theme_few()
```
## Conclusions for Research Question 2:

From our analysis of reducing the dimensionality reduction of our data, it seems as though there are two main groupings within our data. The first is the price of the home. The sale price of a home is highly correlated with many other variables in the dataset and knowing the sale price can give you a good idea about other details of the home. The second main grouping is the amount of lot area/PUD status of a home. This grouping likely is a proxy for measuring how suburban a home is, because suburban homes are often PUDs that often have much more land then their city counterparts.

The price and suburban-ness of a home make sense as good metrics for explaining the variation between homes because an expensive loft in the city is quite different than an expensive suburban home, as is an inexpensive suburban home from a inexpensive urban home. 
