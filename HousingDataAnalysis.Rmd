---
title: "Housing Data Analysis"
author: "Brooke Fitzgerald, Gloria Giramahoro, Sergi Drago"
date: "4/2/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gpairs)
library(ggthemes)
library(dplyr)
library(Rtsne)
library(amap)
library(dummies)
library(rpart)
library(caret)
```

## Loading the data

```{r importdata}
fullTrain <- read.csv("train.csv")
fullTest <- read.csv("test.csv")
```

## Exploratory Data Analysis

```{r pressure, echo=FALSE}
summary(fullTrain)
ggplot(fullTrain, aes(x=Neighborhood,y=SalePrice))+
       geom_boxplot()+
       theme_few()+
       theme(axis.text.x = element_text(angle = 90, hjust = 1))
options(scipen=5)
hist(fullTrain$SalePrice, xlab = "Sale Price", main = "Histogram of Sale Price")
```

To do a preliminary examination of the data, we wanted to do some dimensionality reduction to see if there were any obvious structures in the data.

First, we computed PCA on the data, then plotted them. 
```{r}
numeric_columns_only <- fullTrain %>% select_if(is.numeric)
numeric_full_columns_only <- numeric_columns_only[complete.cases(numeric_columns_only),]

p<-pca(as.matrix(numeric_full_columns_only))

x<-princomp(as.matrix(numeric_full_columns_only))

numeric_col_w_prcmp <- numeric_full_columns_only %>% mutate(pca1 = x$scores[,1],pca2 = x$scores[,2])

cols_w_prcmp <- inner_join(numeric_col_w_prcmp, fullTrain)

outlier <- cols_w_prcmp %>% filter(pca1>150000)

cols_w_prcmp <- cols_w_prcmp %>% filter(pca1<150000)

ggplot(cols_w_prcmp, aes(x=pca1, y=pca2, col=MSSubClass))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
```

Then we decided to compare dimensionality reduction of a t-SNE plot, which stands for t-distributed Stochastic Neighbor Embedding. It is a dimensionality reduction algorithm that embeds high dimensional data into a 2 dimensional space. There is a nice [R vignette about the package](https://cran.r-project.org/web/packages/tsne/tsne.pdf), a [blog post](https://www.codeproject.com/Tips/788739/Visualization-of-High-Dimensional-Data-using-t-SNE) about its use in R, and an [interactive visualization](http://distill.pub/2016/misread-tsne/) characterizing its strengths and weaknesses.

```{r}
housing_tsne<-Rtsne(numeric_full_columns_only)

numeric_col_w_tsne<- numeric_full_columns_only %>% mutate(tsne1 = housing_tsne$Y[,1],tsne2 = housing_tsne$Y[,2])

cols_w_tsne <- inner_join(numeric_col_w_tsne, fullTrain)

ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=log(SalePrice)))+
       geom_point()+
       theme_few()
ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=MSSubClass))+
       geom_point()+
       theme_few()
```

Next we wanted to do some one hot variable encoding in order to test correlation between SalePrice and factor variables.
```{r}
one_hot_encoded<- dummy.data.frame(fullTrain)
one_hot_encoded <- one_hot_encoded[complete.cases(one_hot_encoded),]

corprice<-cor(one_hot_encoded)[,"SalePrice"]
corprice<-corprice[!is.na(corprice)]
corprice<-corprice[names(corprice)!="SalePrice"]
sub_corprice <- corprice[abs(corprice)>0.5]

ggplot()+
  geom_bar(aes(x=reorder(names(sub_corprice), -sub_corprice),sub_corprice),stat="identity")+
  theme_few()+
  labs(y="Correlation to SalePrice", x = "Variable")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(one_hot_encoded, aes(x=SalePrice,y=GrLivArea))+
  geom_point()+
  theme_few()
one_hot_encoded %>% filter(GrLivArea>4500)
fullTrain %>% filter(GrLivArea>4500)
```
```{r}
housing_tsne<-Rtsne(one_hot_encoded)

numeric_col_w_tsne<- one_hot_encoded %>% mutate(tsne1 = housing_tsne$Y[,1],tsne2 = housing_tsne$Y[,2])

ggplot(numeric_col_w_tsne, aes(x=tsne1,y=tsne2,col=log(GrLivArea)))+
       geom_point()+
       theme_few()
names(fullTrain)
```
## Starting Predictive Modeling
The first step in predictive modeling is splitting the data into training and test sets.

```{r}
indices<- createDataPartition(fullTrain$SalePrice,p=0.75)$Resample1
train<- fullTrain[indices,]
test<-fullTrain[-indices,]

one_hot_train<- one_hot_encoded[indices,]
one_hot_test<-one_hot_encoded[-indices,]
```

```{r}
tree.model1 <-rpart(SalePrice~.,train)
tree.preds1 <- predict(tree.model1, test)
tree.RMSE1 <- sqrt(sum((tree.preds1-test$SalePrice)^2))

tree.model2 <-rpart(SalePrice~.,one_hot_train)
tree.preds2 <- predict(tree.model2, one_hot_test)
tree.RMSE2 <- sqrt(sum((tree.preds2-one_hot_test$SalePrice)^2))

rpart.plot::prp(tree.model1)
rpart.plot::prp(tree.model2)

[c(-6,-7,)]
my_formula<- as.formula(paste("SalePrice~", paste(names(fullTrain)[73:75], collapse="+")))
lr.model2 <- lm(my_formula,fullTrain)

lr.model1 <- lm(SalePrice~.,one_hot_encoded, na.action = na.omit)
summary(fullTrain)
summary(lr.model1)
summary(lr.model2)
```
