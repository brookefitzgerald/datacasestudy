---
title: "Housing Data Analysis"
author: "Brooke Fitzgerald, Gloria Giramahoro, Sergi Drago"
date: "4/2/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gpairs)
library(ggthemes)
library(dplyr)
library(Rtsne)
library(amap)
library(dummies)
library(rpart)
library(caret)
library(gam)
library(mice)
library(ipred)
library(kknn)

```

## Loading the data

```{r importdata}
fullTrain <- read.csv("train.csv")
fullTest <- read.csv("test.csv")
```

## Exploratory Data Analysis

```{r pressure, echo=FALSE}
summary(fullTrain)
ggplot(fullTrain, aes(x=Neighborhood,y=SalePrice))+
       geom_boxplot()+
       theme_few()+
       theme(axis.text.x = element_text(angle = 90, hjust = 1))
options(scipen=5)
hist(fullTrain$SalePrice, xlab = "Sale Price", main = "Histogram of Sale Price")
```

To do a preliminary examination of the data, we wanted to do some dimensionality reduction to see if there were any obvious structures in the data.

First, we computed PCA on the data, then plotted them. 
```{r}
numeric_columns_only <- fullTrain %>% select_if(is.numeric)
numeric_full_columns_only <- numeric_columns_only[complete.cases(numeric_columns_only),]

p<-pca(as.matrix(numeric_full_columns_only))

x<-princomp(as.matrix(numeric_full_columns_only))

numeric_col_w_prcmp <- numeric_full_columns_only %>% mutate(pca1 = x$scores[,1],pca2 = x$scores[,2])

cols_w_prcmp <- inner_join(numeric_col_w_prcmp, fullTrain)

outlier <- cols_w_prcmp %>% filter(pca1>150000)

cols_w_prcmp <- cols_w_prcmp %>% filter(pca1<150000)

ggplot(cols_w_prcmp, aes(x=pca1, y=pca2, col=MSSubClass))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
```

Then we decided to compare dimensionality reduction of a t-SNE plot, which stands for t-distributed Stochastic Neighbor Embedding. It is a dimensionality reduction algorithm that embeds high dimensional data into a 2 dimensional space. There is a nice [R vignette about the package](https://cran.r-project.org/web/packages/tsne/tsne.pdf), a [blog post](https://www.codeproject.com/Tips/788739/Visualization-of-High-Dimensional-Data-using-t-SNE) about its use in R, and an [interactive visualization](http://distill.pub/2016/misread-tsne/) characterizing its strengths and weaknesses.

```{r}
housing_tsne<-Rtsne(numeric_full_columns_only)

numeric_col_w_tsne<- numeric_full_columns_only %>% mutate(tsne1 = housing_tsne$Y[,1],tsne2 = housing_tsne$Y[,2])

cols_w_tsne <- inner_join(numeric_col_w_tsne, fullTrain)

ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=log(SalePrice)))+
       geom_point()+
       theme_few()
ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=MSSubClass))+
       geom_point()+
       theme_few()
```

Next we wanted to do some one hot variable encoding in order to test correlation between SalePrice and factor variables.
```{r}
one_hot_encoded<- dummy.data.frame(fullTrain)
one_hot_encoded_cc <- one_hot_encoded[complete.cases(one_hot_encoded),]

corprice<-cor(one_hot_encoded_cc)[,"SalePrice"]
corprice<-corprice[names(corprice)!="SalePrice"]
sub_corprice <- corprice[abs(corprice)>0.5]

ggplot()+
  geom_bar(aes(x=reorder(names(sub_corprice), -sub_corprice),sub_corprice),stat="identity")+
  theme_few()+
  labs(y="Correlation to SalePrice", x = "Variable")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(one_hot_encoded_cc, aes(x=SalePrice,y=GrLivArea))+
  geom_point()+
  theme_few()
```
```{r}
housing_tsne<-Rtsne(one_hot_encoded_cc)

numeric_col_w_tsne<- one_hot_encoded_cc %>% mutate(tsne1 = housing_tsne$Y[,1],tsne2 = housing_tsne$Y[,2])

ggplot(numeric_col_w_tsne, aes(x=tsne1,y=tsne2,col=log(GrLivArea)))+
       geom_point()+
       theme_few()
```

## Removing NA values
Because some predictive models don't allow for NA values and because every single observation has at least one NA value, we want to do some data imputation. The data was imputed using random forests. This imputed data can be used in conjunction with the one-hot encoded data to compare and contrast the various predictions.

```{r, echo=F}
imp <- mice(fullTrain,method='rf')
imputed <- fullTrain

# This function takes in a row of categories, then returns the category occuring most often
f<- function(dataRow){
       t<-table(dataRow)
       return(names(sort(t,decreasing = TRUE)[1]))
}
i=1
for(varName in imp$imp){
       if (!is.null(varName[[1]])){
              if (is.numeric(varName[[1]])){
                    # if data is numeric, impute the mean 
                     imputed[row.names(varName),names(imputed)[i]] = round(rowMeans(varName))
              } else {
                    # if data is categorical, impute the most often predicted category
                     imputed[row.names(varName),names(imputed)[i]] = apply(varName, 1, f)
              }
              
       }
       i<-i+1
}

```
## Starting Predictive Modeling
The first step in predictive modeling is splitting the data into training and test sets.

```{r}
indices<- createDataPartition(fullTrain$SalePrice,p=0.75)$Resample1
train.full<- fullTrain[indices,]
test.full<-fullTrain[-indices,]

train.imp<- imputed[indices,]
test.imp<-imputed[-indices,]

train.onehot<- one_hot_encoded[indices,]
test.onehot<-one_hot_encoded[-indices,]
```

The first model we tried out was a decision tree.

```{r}
tree.model.full <-rpart(SalePrice~.,train.full)
tree.preds.full <- predict(tree.model.full, test.full)
tree.RMSE.full <- sqrt(sum((tree.preds1-test.full$SalePrice)^2))/length(tree.preds.full)
tree.RMSE.full

tree.model.onehot <-rpart(SalePrice~.,train.onehot)
tree.preds.onehot <- predict(tree.model.onehot, test.onehot)
tree.RMSE.onehot <- sqrt(sum((tree.preds.onehot-test.onehot$SalePrice)^2))/length(tree.preds.onehot)
tree.RMSE.onehot

tree.model.imp <-rpart(SalePrice~.,train.imp)
tree.preds.imp <- predict(tree.model.imp, test.imp)
tree.RMSE.imp <- sqrt(sum((tree.preds.imp-test.imp$SalePrice)^2))/length(tree.preds.imp)
tree.RMSE.imp

tree.preds.avg <- rowMeans(cbind(tree.preds.full,tree.preds.onehot,tree.preds.imp))
tree.RMSE.avg <- sqrt(sum((tree.preds.avg-test.full$SalePrice)^2))/length(tree.preds.avg)
tree.RMSE.avg

rpart.plot::prp(tree.model.full)
rpart.plot::prp(tree.model.onehot)
rpart.plot::prp(tree.model.imp)

#[c(-6,-7,)]

#my_formula<- as.formula(paste("SalePrice~", paste(names(fullTrain)[73:75], collapse="+")))
gam.model2 <- gam(SalePrice~.,data=fullTrain)

lr.model.imp <- lm(SalePrice~.,train.imp)
lr.model.imp$xlevels$Condition2 <- union(lr.model.imp$xlevels$Condition2, levels(test.imp$Condition2))
lr.model.imp$xlevels$Exterior1st <- union(lr.model.imp$xlevels$Exterior1st, levels(test.imp$Exterior1st))
lr.preds.imp <- predict(lr.model.imp,test.imp)
lr.RMSE.imp <- sqrt(sum((lr.preds.imp-test.imp$SalePrice)^2))/length(lr.preds.imp)
lr.RMSE.imp

lr.model.onehot <- lm(SalePrice~.,train.onehot, na.action = na.omit)
lr.preds.onehot <- predict(lr.model.onehot,test.onehot)
lr.RMSE.onehot <- sqrt(sum((lr.preds.onehot-test.onehot$SalePrice)^2))/length(lr.preds.onehot)
lr.RMSE.onehot

summary(lr.model.onehot)
```
# bagging regressionof Salesprice: nbagg =25(bbotstrap replications)
```{r}
fullTrain$Id <- NULL
fullTrain.bagging<-bagging(SalePrice~.,data=fullTrain,coob=TRUE,na.action = na.omit)
print(fullTrain.bagging)
summary(fullTrain.bagging)
fullTrain_SalePrice <- predict(fullTrain.bagging, 
                                         fullTrain)
summary(fullTrain_SalePrice)
 

```


# KNN regression
```{r}
train.onehot<- train.onehot[complete.cases(train.onehot),]
test.onehot<- test.onehot[complete.cases(test.onehot),]
fullTrain_knn<- knnreg(train.onehot[-306],train.onehot$SalePrice, k=3);
plot(test.onehot$SalePrice,predict(fullTrain_knn,test.onehot[-306]))
summary


```
In order to test the significance of variables
```{r}
lm_1 <- lm(train.onehot$SalePrice ~ , data = train.onehot )
summary(lm_1)
summary(lm_1)$fstatistic
# The significance measure we want:
summary(lm_1)$fstatistic[1]
#We need a vector to hold the outputs to avoid working with them one at a time.
exp_var_fstat <- as.numeric(rep(NA, times = 30))
names(exp_var_fstat) <- names(train.onehot)
#Now run through the variables, get the linear fit and store the F-statistic


## Hierarchical clustering
```{r}
par(mar=rep(1, 4))
d <- dist(one_hot_encoded_cc, method = "euclidean") # distance matrix
hclust.fit <- hclust(d, method = "ward.D")
plot(hclust.fit) # display dendogram
hclust.groups <- cutree(hclust.fit, k = 5)
pairs(one_hot_encoded_cc$SalePrice, col = hclust.groups, main = "Hierarchical Cluster")
plot(one_hot_encoded_cc$SalePrice~., xlim = c(-0.158, 2), xlab = "variables other than SalePrice", ylab = "SalePrice",col = hclust.groups, main = "Hierarchical Cluster", pch = 20)
par(opar)
```
```{r}
require(graphics)
hc <- hclust(dist(one_hot_encoded_cc),"ave")
plot(hc)
plot(hc, hang = -1)

## Do the same with centroid clustering and *squared* Euclidean distance,
## cut the tree into ten clusters and reconstruct the upper part of the
## tree from the cluster centers.
hc <- hclust(dist(one_hot_encoded_cc)^2, "cen")
memb <- cutree(hc, k = 3)
cent <- NULL
for (k in 1:3) {
  cent <- rbind(cent, colMeans(one_hot_encoded_cc[memb == k,]))
}
hc1 <- hclust(dist(cent)^2, method = "cen", members = table(memb))
opar <- par(mfrow = c(1, 2))
plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
par(opar)



```


                                  