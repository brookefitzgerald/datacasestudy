q()
load("C:/Users/Gloria/Downloads/BostonHousing2.rda")
setwd("~/GitHub/datacasestudy")
load("C:/Users/Gloria/Documents/HousingDataAnalysis.Rmd")
load("C:/Users/Gloria/Documents/HousingDataAnalysis.rdta")
load("C:/Users/Gloria/Documents/HousingDataAnalysis.rda")
install.packages("gpairs")
install.packages("ggthemes")
load("C:/Users/Gloria/Downloads/al2.Rdata")
View(BostonHousing2)
View(BostonHousing2)
fullTrain <- read.csv("train.csv")
View(fullTrain)
library(ggplot2)
library("grid", lib.loc="C:/Program Files/R/R-3.4.0/library")
install install.packages("ggplot2")
install install.packages(ggplot2)
install.packages(ggplot2)
install.packages("ggplot2")
library(ggplot2)
library(gpairs)
library(ggplot2)
library(gpairs)
install.packages("gpairs")
install.packages(ggthemes)
library(ggplot2)
library(gpairs)
library(ggthemes)
install.packages("ggthemes")
library(ggplot2)
library(gpairs)
library(ggthemes)
library(dplyr)
install.packages("dplyr")
library(ggplot2)
library(gpairs)
library(ggthemes)
library(dplyr)
library(Rtsne)
install.packages("Rtsne")
library(ggplot2)
library(gpairs)
library(ggthemes)
library(dplyr)
library(Rtsne)
fullTrain <- read.csv("train.csv")
fullTest <- read.csv("test.csv")
summary(fullTrain)
ggplot(fullTrain, aes(x=Neighborhood,y=SalePrice))+
geom_boxplot()+
theme_few()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(fullTrain, aes(x=Neighborhood,y=SalePrice))+
geom_boxplot()+
theme_few()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
options(scipen=5)
hist(fullTrain$SalePrice, xlab = "Sale Price", main = "Histogram of Sale Price")
install packages("Rpart")
install packages(rpart)
install.packages("rpart")
library(rpart)
install.packages("ipred")
fullTrain <- fullTrain[-1]
fullTrain$Id <- NULL
bagging(Class~ SalePrice+MSSubClass+MSZoning+LotFrontage+LotArea+Street+Alley+SaleType+SaleCondition,data=fullTrain, coob=TRUE)
library(rpart)
library(ipred)
bagging(Class~ SalePrice+MSSubClass+MSZoning+LotFrontage+LotArea+Street+Alley+SaleType+SaleCondition,data=fullTrain, coob=TRUE)
bagging(SalePrice~ +MSSubClass+MSZoning+LotFrontage+LotArea+Street+Alley+SaleType+SaleCondition,data=fullTrain, coob=TRUE)
bagging(Class~ SalePrice+MSSubClass+MSZoning+LotFrontage+LotArea+Street+Alley+SaleType+SaleCondition+YrSold,data=fullTrain, coob=TRUE)
bagging(SalePrice~ MSSubClass+MSZoning+LotFrontage+LotArea+Street+Alley+SaleType+SaleCondition+YrSold,data=fullTrain, coob=TRUE)
class(fullTrain$YrSold)
class(fullTrain$SalePrice)
fullTrain$SalePrice <- factor(fullTrain$SalePrice)
fullTrain.bagging <- bagging(SalePrice ~.,
data=fullTrain,
mfinal=15,
control=rpart.control(maxdepth=5, minsplit=15))
fullTrain.bagging <- bagging(SalePrice ~.,
data=fullTrain,
mfinal=15,
control=rpart.control(maxdepth=5, minsplit=15),data.na= False)
table(head(fullTrain))
kable(head(fullTrain))
fullTrain <- fullTrain[!is.na(fullTrain)]
table(head(fullTrain))
fullTrain.bagging <- bagging(SalePrice ~.,
data=fullTrain,
mfinal=15,data.na= False)
fullTrain.bagging <- bagging(SalePrice ~.,
data=fullTrain,data.na= False)
summary(fullTrain)
fullTrain.bagging <- bagging(SalePrice ~.,
data=fullTrain,data.na= False)
summary(fullTrain)
fullTrain <- read.csv("train.csv")
summary(fullTrain)
class(MSSubClass)
class(MSZoning)
class(LotFrontage)
class(LotArea)
class(Street)
class(Alley )
class(LotShape)
class(LandContour)
class(Utilities)
class(LotConfig)
class(LandSlope)
class(Neighborhood)
class(Condition1)
class(fullTrain$MSSubClass)
class(fullTrain$MSZoning)
class(fullTrain$LotFrontage)
class(fullTrain$LotArea)
class(fullTrain$Street)
class(fullTrain$Alley )
class(fullTrain$LotShape)
class(fullTrain$LandContour)
class(fullTrain$Utilities)
class(fullTrain$LotConfig)
class(fullTrain$LandSlope)
class(fullTrain$Neighborhood)
class(fullTrain$Condition1)
class(fullTrain$GarageQual)
fullTrain.bagging <- bagging(SalePrice ~LotArea+LotFrontage+MSSubClass,
data=fullTrain,data.na= False)
fullTrain.bagging <- bagging(SalePrice ~LotArea+LotFrontage+MSSubClass,
data=fullTrain)
fullTrain.bagging <- bagging(SalePrice ~LotArea+LotFrontage+MSSubClass,
data=fullTrain,coob=TRUE)
SalePrice$pred.class <- predict(fullTrain.bagging,
SalePrice)
fullTrain$ SalePrice$pred.class <- predict(fullTrain.bagging,
SalePrice)
fullTrain$ SalePrice$pred.class <- predict(fullTrain.bagging,
fullTrain$SalePrice)
fullTrain.bagging <- bagging(SalePrice ~LotArea+LotFrontage+MSSubClass,data=fullTrain,coob=TRUE)
fullTrain$ SalePrice$pred.class <- predict(fullTrain.bagging,
fullTrain$SalePrice)
install.packages("kknn")
library(kknn)
summary(fullTrain)
model <- fullTrain.kknn(SalePrice ~ ., data = learning, kmax = 1459)
View(fullTest)
View(fullTest)
model <- fullTrain.kknn(SalePrice ~ ., data = learning, kmax = 1459)
suppressWarnings(suppressMessages(library(kknn)))
model <- train.kknn(SalePrice ~ ., data = fullTrain, kmax = 1459)
model <- train.kknn(SalePrice ~ ., data = fullTrain, kmax = 1459,data.na= False)
fullTrain$Default <- factor(fullTrain$Default)
numericalVariable<- sapply(fullTrain, is.numeric)
num_var<- sapply(fullTrain, is.numeric)
fullTrain[num_var] <- lapply(fullTrain[num_var], scale)
summary(fullTrain[num_var])
##The kNN Algorithm
library(kknn)
## Convert the dependent var to factor. Normalize the numeric variables
fullTrain$Id<- factor(fullTrain$Id)
num_var<- sapply(fullTrain, is.numeric)
fullTrain[num_var] <- lapply(fullTrain[num_var], scale)
summary(fullTrain[num_var])
## Let's predict on a test set of 1000 observations.The Rest will be used as train set.
set.seed(1234)
test <- 1:1000
train.fullTrain <-fullTrain.subset[-test,]
test.fullTrain <- fullTrain.subset[test,]
train.Id <- fullTrain$Id[-test]
test.Id <- fullTrain$Id[test]
## Let's assume k values= 1, 90 and 200 to see how they perform in terms of correct proportion of classification and success rate. The optimum k value can be chosen based on the outcomes
install.packages(class)
library(class)
knn.1 <-  knn(train.fullTrain, test.fullTrain, train.Id, k=1)
knn.90 <-  knn(train.fullTrain, test.fullTrain, train.Id, k=90)
knn.200 <- knn(train.fullTrain, test.fullTrain, train.Id, k=200)
## Let's calculate the percentage of correct classification for k = 1,90 & 200
100 * sum(test.Id == knn.1)/1000  # For knn = 1
##The kNN Algorithm
library(kknn)
## Convert the dependent var to factor. Normalize the numeric variables
fullTrain$Id<- factor(fullTrain$Id)
num_var<- sapply(fullTrain, is.numeric)
fullTrain[num_var] <- lapply(fullTrain[num_var], scale)
summary(fullTrain[num_var])
## Let's predict on a test set of 1000 observations.The Rest will be used as train set.
set.seed(1234)
test <- 1:1000
train.fullTrain <-fullTrain[num_var][-test,]
test.fullTrain <- fullTrain[num_var][test,]
train.Id <- fullTrain$Id[-test]
test.Id <- fullTrain$Id[test]
## Let's assume k values= 1, 90 and 200 to see how they perform in terms of correct proportion of classification and success rate. The optimum k value can be chosen based on the outcomes
install.packages(class)
library(class)
knn.1 <-  knn(train.fullTrain, test.fullTrain, train.Id, k=1)
knn.90 <-  knn(train.fullTrain, test.fullTrain, train.Id, k=90)
knn.200 <- knn(train.fullTrain, test.fullTrain, train.Id, k=200)
## Let's calculate the percentage of correct classification for k = 1,90 & 200
100 * sum(test.Id == knn.1)/1000  # For knn = 1
##The kNN Algorithm
library(kknn)
## Convert the dependent var to factor. Normalize the numeric variables
fullTrain$Id<- factor(fullTrain$Id)
num_var<- sapply(fullTrain, is.numeric)
fullTrain[num_var] <- lapply(fullTrain[num_var], scale)
summary(fullTrain[num_var])
## Let's predict on a test set of 1000 observations.The Rest will be used as train set.
set.seed(1234)
test <- 1:1000
train.fullTrain <-fullTrain[num_var][-test,]
test.fullTrain <- fullTrain[num_var][test,]
train.Id <- fullTrain$Id[-test]
test.Id <- fullTrain$Id[test]
## Let's assume k values= 1, 90 and 200 to see how they perform in terms of correct proportion of classification and success rate. The optimum k value can be chosen based on the outcomes
install.packages(class)
library(class)
knn.1 <-  knn(train.fullTrain, test.fullTrain, train.Id,data.na= FALSE, k=1)
knn.90 <-  knn(train.fullTrain, test.fullTrain, train.Id,data.na= FALSE, k=90)
knn.200 <- knn(train.fullTrain, test.fullTrain, train.Id,data.na= FALSE,k=200)
## Let's calculate the percentage of correct classification for k = 1,90 & 200
100 * sum(test.Id == knn.1)/1000  # For knn = 1
##The kNN Algorithm
library(kknn)
## Convert the dependent var to factor. Normalize the numeric variables
fullTrain$Id<- factor(fullTrain$Id)
num_var<- sapply(fullTrain, is.numeric)
fullTrain[num_var] <- lapply(fullTrain[num_var], scale)
summary(fullTrain[num_var])
## Let's predict on a test set of 1000 observations.The Rest will be used as train set.
set.seed(1234)
test <- 1:1000
train.fullTrain <-fullTrain[num_var][-test,]
test.fullTrain <- fullTrain[num_var][test,]
train.Id <- fullTrain$Id[-test]
test.Id <- fullTrain$Id[test]
## Let's assume k values= 1, 90 and 200 to see how they perform in terms of correct proportion of classification and success rate. The optimum k value can be chosen based on the outcomes
install.packages(class)
library(class)
knn.1 <-  knn(train.fullTrain, test.fullTrain, train.Id,data.na= TRUE, k=1)
knn.90 <-  knn(train.fullTrain, test.fullTrain, train.Id,data.na= TRUE, k=90)
knn.200 <- knn(train.fullTrain, test.fullTrain, train.Id,data.na= TRUE,k=200)
## Let's calculate the percentage of correct classification for k = 1,90 & 200
100 * sum(test.Id == knn.1)/1000  # For knn = 1
##The kNN Algorithm
library(kknn)
## Convert the dependent var to factor. Normalize the numeric variables
fullTrain$Id<- factor(fullTrain$Id)
num_var<- sapply(fullTrain, is.numeric)
fullTrain[num_var] <- lapply(fullTrain[num_var], scale)
summary(fullTrain[num_var])
## Let's predict on a test set of 1000 observations.The Rest will be used as train set.
set.seed(1234)
test <- 1:1000
train.fullTrain <-fullTrain[num_var][-test,]
test.fullTrain <- fullTrain[num_var][test,]
train.Id <- fullTrain$Id[-test]
test.Id <- fullTrain$Id[test]
## Let's assume k values= 1, 90 and 200 to see how they perform in terms of correct proportion of classification and success rate. The optimum k value can be chosen based on the outcomes
install.packages(class)
library(class)
knn.1 <-  knn(train.fullTrain, test.fullTrain, train.Id,na.rm= TRUE, k=1)
knn.90 <-  knn(train.fullTrain, test.fullTrain, train.Id,na.rm= TRUE, k=90)
knn.200 <- knn(train.fullTrain, test.fullTrain, train.Id,na.rm= TRUE,k=200)
## Let's calculate the percentage of correct classification for k = 1,90 & 200
100 * sum(test.Id == knn.1)/1000  # For knn = 1
fullTrain.new<- sapply(fullTrain,is.numeric)
summary(fullTrain.new)
fullTrain.new<- sapply(fullTrain,is.numeric)
fullTrain.subset<-fullTrain[ , fullTrain.new]
summary(fullTrain.subset)
summary(fullTrain.subset)
fullTrain <- read.csv("train.csv")
summary(fullTrain)
View(fullTrain.subset)
View(fullTrain)
View(fullTrain)
fullTest <- read.csv("test.csv")
View(fullTest)
load(fullTrain)
load(fullTrain)
View(fullTest)
View(fullTest)
