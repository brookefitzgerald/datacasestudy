---
title: "Housing Data Analysis"
author: "Brooke Fitzgerald, Gloria Giramahoro, Sergi Drago"
date: "4/2/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gpairs)
library(ggthemes)
library(MASS)
library(dplyr)
library(Rtsne)
library(amap)
library(dummies)
library(rpart)
library(caret)
library(gam)
library(mice)
library(ipred)
library(kknn)
require(graphics)
library(rattle)
library(gridExtra)
library(leaps)
library(glmnet)
```

## Loading and cleaning the data

```{r importdata}
fullTrain <- read.csv("train.csv")
fullTest <- read.csv("test.csv")

fullTrain$LotFrontage[is.na(fullTrain$LotFrontage)]<-0.0

levels(fullTrain$Alley)[length(levels(fullTrain$Alley))+1] = "None"
fullTrain$Alley[is.na(fullTrain$Alley)]<-"None"

levels(fullTrain$PoolQC)[length(levels(fullTrain$PoolQC))+1] = "None"
fullTrain$PoolQC[is.na(fullTrain$PoolQC)]<-"None"

levels(fullTrain$Fence)[length(levels(fullTrain$Fence))+1] = "None"
fullTrain$Fence[is.na(fullTrain$Fence)]<-"None"

levels(fullTrain$MiscFeature)[length(levels(fullTrain$MiscFeature))+1] = "None"
fullTrain$MiscFeature[is.na(fullTrain$MiscFeature)]<-"None"

levels(fullTrain$GarageQual)[length(levels(fullTrain$GarageQual))+1] = "None"
fullTrain$GarageQual[is.na(fullTrain$GarageQual)]<-"None"

levels(fullTrain$GarageCond)[length(levels(fullTrain$GarageCond))+1] = "None"
fullTrain$GarageCond[is.na(fullTrain$GarageCond)]<-"None"

levels(fullTrain$BsmtQual)[length(levels(fullTrain$BsmtQual))+1] = "None"
fullTrain$BsmtQual[is.na(fullTrain$BsmtQual)]<-"None"

levels(fullTrain$BsmtCond)[length(levels(fullTrain$BsmtCond))+1] = "None"
fullTrain$BsmtCond[is.na(fullTrain$BsmtCond)]<-"None"

levels(fullTrain$BsmtExposure)[length(levels(fullTrain$BsmtExposure))+1] = "None"
fullTrain$BsmtExposure[is.na(fullTrain$BsmtExposure)]<-"None"

levels(fullTrain$BsmtFinType1)[length(levels(fullTrain$BsmtFinType1))+1] = "None"
fullTrain$BsmtFinType1[is.na(fullTrain$BsmtFinType1)]<-"None"

levels(fullTrain$BsmtFinType2)[length(levels(fullTrain$BsmtFinType2))+1] = "None"
fullTrain$BsmtFinType2[is.na(fullTrain$BsmtFinType2)]<-"None"

levels(fullTrain$FireplaceQu)[length(levels(fullTrain$FireplaceQu))+1] = "None"
fullTrain$FireplaceQu[is.na(fullTrain$FireplaceQu)]<-"None"

levels(fullTrain$GarageYrBlt)[length(levels(fullTrain$GarageYrBlt))+1] = "None"
fullTrain$GarageYrBlt[is.na(fullTrain$GarageYrBlt)]<-"None"

levels(fullTrain$GarageType)[length(levels(fullTrain$GarageType))+1] = "None"
fullTrain$GarageType[is.na(fullTrain$GarageType)]<-"None"

levels(fullTrain$GarageFinish)[length(levels(fullTrain$GarageFinish))+1] = "None"
fullTrain$GarageFinish[is.na(fullTrain$GarageFinish)]<-"None"

dim(fullTrain[complete.cases(fullTrain),])
```
```{r,echo=FALSE}
fullTest$LotFrontage[is.na(fullTest$LotFrontage)]<-0.0

levels(fullTest$Alley)[length(levels(fullTest$Alley))+1] = "None"
fullTest$Alley[is.na(fullTest$Alley)]<-"None"

levels(fullTest$PoolQC)[length(levels(fullTest$PoolQC))+1] = "None"
fullTest$PoolQC[is.na(fullTest$PoolQC)]<-"None"

levels(fullTest$Fence)[length(levels(fullTest$Fence))+1] = "None"
fullTest$Fence[is.na(fullTest$Fence)]<-"None"

levels(fullTest$MiscFeature)[length(levels(fullTest$MiscFeature))+1] = "None"
fullTest$MiscFeature[is.na(fullTest$MiscFeature)]<-"None"

levels(fullTest$GarageQual)[length(levels(fullTest$GarageQual))+1] = "None"
fullTest$GarageQual[is.na(fullTest$GarageQual)]<-"None"

levels(fullTest$GarageCond)[length(levels(fullTest$GarageCond))+1] = "None"
fullTest$GarageCond[is.na(fullTest$GarageCond)]<-"None"

levels(fullTest$BsmtQual)[length(levels(fullTest$BsmtQual))+1] = "None"
fullTest$BsmtQual[is.na(fullTest$BsmtQual)]<-"None"

levels(fullTest$BsmtCond)[length(levels(fullTest$BsmtCond))+1] = "None"
fullTest$BsmtCond[is.na(fullTest$BsmtCond)]<-"None"

levels(fullTest$BsmtExposure)[length(levels(fullTest$BsmtExposure))+1] = "None"
fullTest$BsmtExposure[is.na(fullTest$BsmtExposure)]<-"None"

levels(fullTest$BsmtFinType1)[length(levels(fullTest$BsmtFinType1))+1] = "None"
fullTest$BsmtFinType1[is.na(fullTest$BsmtFinType1)]<-"None"

levels(fullTest$BsmtFinType2)[length(levels(fullTest$BsmtFinType2))+1] = "None"
fullTest$BsmtFinType2[is.na(fullTest$BsmtFinType2)]<-"None"

levels(fullTest$FireplaceQu)[length(levels(fullTest$FireplaceQu))+1] = "None"
fullTest$FireplaceQu[is.na(fullTest$FireplaceQu)]<-"None"

levels(fullTest$GarageType)[length(levels(fullTest$GarageType))+1] = "None"
fullTest$GarageType[is.na(fullTest$GarageType)]<-"None"

levels(fullTest$GarageFinish)[length(levels(fullTest$GarageFinish))+1] = "None"
fullTest$GarageFinish[is.na(fullTest$GarageFinish)]<-"None"

fullTest<-dplyr::select(fullTest,-GarageYrBlt)
fullTrain<-dplyr::select(fullTrain,-GarageYrBlt)
```

## Exploratory Data Analysis

```{r}
summary(fullTrain)

ggplot(fullTrain, aes(x=Neighborhood,y=SalePrice))+
       geom_boxplot()+
       theme_few()+
       theme(axis.text.x = element_text(angle = 90, hjust = 1))
options(scipen=5)
hist(fullTrain$SalePrice, xlab = "Sale Price", main = "Histogram of Sale Price")

ggplot(fullTrain, aes(x=YearBuilt,y=SalePrice))+
       geom_point(alpha = 0.4)+
       theme_few()+
       theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(fullTrain, aes(x=OverallQual,y=SalePrice))+
       geom_jitter(alpha = 0.5)+
       theme_few()
ggplot(fullTrain, aes(x=TotRmsAbvGrd,y=SalePrice, col = OverallQual))+
       geom_jitter(alpha = 0.7)+
       theme_few()
fullTrain %>% group_by(MoSold) %>% summarize(NumSold=n(),MeanSalePrice=mean(SalePrice))%>%
ggplot(aes(x=MoSold,y=NumSold, fill=MeanSalePrice))+
       geom_bar(stat='identity')+
       scale_x_continuous(breaks = 1:12)+
       theme_few()
fullTrain %>% group_by(MoSold) %>% summarize(NumSold=n(),MedianSalePrice=median(SalePrice))%>%
ggplot(aes(x=MoSold,y=NumSold, fill=MedianSalePrice))+
       geom_bar(stat='identity')+
       scale_x_continuous(breaks = 1:12)+
       theme_few()

```

To do a preliminary examination of the data, we wanted to do some dimensionality reduction to see if there were any obvious structures in the data.

First, we computed PCA on the data with two different algorithms, then plotted them. 
```{r}
numeric_columns_only <- fullTrain %>% select_if(is.numeric)
numeric_full_columns_only <- numeric_columns_only[complete.cases(numeric_columns_only),]

p<-pca(as.matrix(numeric_full_columns_only))

x<-princomp(as.matrix(numeric_full_columns_only))

numeric_col_w_prcmp <- numeric_full_columns_only %>% mutate(princomp1 = x$scores[,1], princomp2 = x$scores[,2], pca1 = p$scores[,1], pca2 = p$scores[,2])

cols_w_prcmp <- inner_join(numeric_col_w_prcmp, fullTrain)

outlier <- cols_w_prcmp %>% filter(princomp2<(-10000))

cols_w_prcmp <- cols_w_prcmp %>% filter(princomp2>(-10000))

ggplot(cols_w_prcmp, aes(x=pca1, y=pca2, col=MSSubClass))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
ggplot(cols_w_prcmp, aes(x=princomp1, y=princomp2, col=MSSubClass))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
```

Then we decided to compare dimensionality reduction of a t-SNE plot, which stands for t-distributed Stochastic Neighbor Embedding. It is a dimensionality reduction algorithm that embeds high dimensional data into a 2 dimensional space. There is a nice [R vignette about the package](https://cran.r-project.org/web/packages/tsne/tsne.pdf), a [blog post](https://www.codeproject.com/Tips/788739/Visualization-of-High-Dimensional-Data-using-t-SNE) about its use in R, and an [interactive visualization](http://distill.pub/2016/misread-tsne/) characterizing its strengths and weaknesses.

```{r}
housing_tsne<-Rtsne(numeric_full_columns_only)

numeric_col_w_tsne<- numeric_full_columns_only %>% mutate(tsne1 = housing_tsne$Y[,1],tsne2 = housing_tsne$Y[,2])

cols_w_tsne <- inner_join(numeric_col_w_tsne, fullTrain)

ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=log(SalePrice)))+
       geom_point()+
       theme_few()
ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=MSSubClass))+
       geom_point()+
       theme_few()
```

Next we wanted to do some one hot variable encoding in order to test correlation between SalePrice and factor variables.
```{r}
one_hot_encoded<- dummy.data.frame(fullTrain)
one_hot_encoded_cc <- one_hot_encoded[complete.cases(one_hot_encoded),]

corprice<-cor(one_hot_encoded_cc)[,"SalePrice"]
corprice<-corprice[names(corprice)!="SalePrice"]
sub_corprice <- corprice[abs(corprice)>0.5]

ggplot()+
  geom_bar(aes(x=reorder(names(sub_corprice), -sub_corprice),sub_corprice),stat="identity")+
  theme_few()+
  coord_flip()+
  labs(y="Correlation to SalePrice", x = "Variable")

ggplot(one_hot_encoded_cc, aes(x=SalePrice,y=GrLivArea))+
  geom_point()+
  theme_few()
```


```{r}
housing_tsne<-Rtsne(one_hot_encoded_cc)

numeric_col_w_tsne<- one_hot_encoded_cc %>% mutate(tsne1 = housing_tsne$Y[,1],tsne2 = housing_tsne$Y[,2])

ggplot(numeric_col_w_tsne, aes(x=tsne1,y=tsne2,col=log(GrLivArea)))+
       geom_point()+
       theme_few()
```

## Removing NA values
Because some predictive models don't allow for NA values and because every single observation has at least one NA value, we want to do some data imputation. The data was imputed using random forests. This imputed data can be used in conjunction with the one-hot encoded data to compare and contrast the various predictions.

```{r, echo=F}
imp <- mice(fullTrain,method='rf')
imputed <- fullTrain

# This function takes in a row of categories, then returns the category occuring most often
f<- function(dataRow){
       t<-table(dataRow)
       return(names(sort(t,decreasing = TRUE)[1]))
}
i=1
for(varName in imp$imp){
       if (!is.null(varName[[1]])){
              if (is.numeric(varName[[1]])){
                    # if data is numeric, impute the mean 
                     imputed[row.names(varName),names(imputed)[i]] = round(rowMeans(varName))
              } else {
                    # if data is categorical, impute the most often predicted category
                     imputed[row.names(varName),names(imputed)[i]] = apply(varName, 1, f)
              }
              
       }
       i<-i+1
}

```
## Starting Predictive Modeling
The first step in predictive modeling is splitting the data into training and test sets.

```{r}
indices<- createDataPartition(fullTrain$SalePrice,p=0.75)$Resample1
train.full<- fullTrain[indices,]
test.full<-fullTrain[-indices,]

train.imp<- imputed[indices,]
test.imp<-imputed[-indices,]

train.onehot<- one_hot_encoded[indices,]
test.onehot<-one_hot_encoded[-indices,]

train.onehot.cc <-train.onehot[complete.cases(train.onehot),]
test.onehot.cc <-test.onehot[complete.cases(test.onehot),]
```

The first model we tried out was a decision tree.

```{r}
tree.model.full <-rpart(SalePrice~.,train.full)
tree.preds.full <- predict(tree.model.full, test.full)
tree.RMSE.full <- sqrt(sum((tree.preds.full-test.full$SalePrice)^2))/length(tree.preds.full)
tree.RMSE.full
p1<-qplot(tree.preds.full,test.full$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Full Regression Tree Predictions",y="Sale Price ")

tree.model.onehot <-rpart(SalePrice~.,train.onehot)
tree.preds.onehot <- predict(tree.model.onehot, test.onehot)
tree.RMSE.onehot <- sqrt(sum((tree.preds.onehot-test.onehot$SalePrice)^2))/length(tree.preds.onehot)
tree.RMSE.onehot
p2<-qplot(tree.preds.onehot,test.onehot$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "One-hot Regression Tree Predictions",y="Sale Price ")

tree.model.imp <- rpart(SalePrice~.,train.imp)
tree.preds.imp <- predict(tree.model.imp, test.imp)
tree.RMSE.imp <- sqrt(sum((tree.preds.imp-test.imp$SalePrice)^2))/length(tree.preds.imp)
tree.RMSE.imp
p3<-qplot(tree.preds.imp,test.imp$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Imputed Regression Tree Predictions",y="Sale Price ")

tree.preds.avg <- rowMeans(cbind(tree.preds.full,tree.preds.onehot,tree.preds.imp))
tree.RMSE.avg <- sqrt(sum((tree.preds.avg-test.full$SalePrice)^2))/length(tree.preds.avg)
tree.RMSE.avg
p4<-qplot(tree.preds.avg,test.full$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Average Regression Tree Predictions",y="Sale Price ")

grid.arrange(p1,p2,p3,p4,ncol=2,nrow=2)

fancyRpartPlot(tree.model.full)
fancyRpartPlot(tree.model.onehot)
fancyRpartPlot(tree.model.imp)

lr.model.full <- lm(SalePrice~.,train.full, na.action = na.omit)
lr.model.full$xlevels$Condition2 <- union(lr.model.full$xlevels$Condition2, levels(test.full$Condition2))
lr.model.full$xlevels$RoofMatl <- union(lr.model.full$xlevels$RoofMatl, levels(test.full$RoofMatl))
lr.model.full$xlevels$BsmtCond <- union(lr.model.full$xlevels$BsmtCond, levels(test.full$BsmtCond))
lr.model.full$xlevels$Heating <- union(lr.model.full$xlevels$Heating, levels(test.full$Heating))
lr.model.full$xlevels$Electrical <- union(lr.model.full$xlevels$Electrical, levels(test.full$Electrical))
lr.model.full$xlevels$MiscFeature <- union(lr.model.full$xlevels$MiscFeature, levels(test.full$MiscFeature))
lr.model.full$xlevels$Exterior1st <- union(lr.model.full$xlevels$Exterior1st, levels(test.full$Exterior1st))
lr.model.full$xlevels$Exterior2nd <- union(lr.model.full$xlevels$Exterior2nd, levels(test.full$Exterior2nd))
lr.model.full$xlevels$Functional <- union(lr.model.full$xlevels$Functional, levels(test.full$Functional))

lr.preds.full <- predict(lr.model.full,test.full)
lr.RMSE.full <- sqrt(sum((lr.preds.full-test.full$SalePrice)^2))/length(lr.preds.full)
lr.RMSE.full
qplot(lr.preds.full,test.full$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Full Linear Model Predictions",y="Full Sale Price ")

lr.model.imp <- lm(SalePrice~.,train.imp)
lr.model.imp$xlevels$Condition2 <- union(lr.model.imp$xlevels$Condition2, levels(test.imp$Condition2))
lr.model.imp$xlevels$Exterior1st <- union(lr.model.imp$xlevels$Exterior1st, levels(test.imp$Exterior1st))
lr.model.imp$xlevels$Exterior2nd <- union(lr.model.imp$xlevels$Exterior2nd, levels(test.imp$Exterior2nd))
lr.model.imp$xlevels$RoofMatl <- union(lr.model.imp$xlevels$RoofMatl, levels(test.imp$RoofMatl))
lr.model.imp$xlevels$Functional <- union(lr.model.imp$xlevels$Functional, levels(test.imp$Functional))
lr.model.imp$xlevels$BsmtCond <- union(lr.model.imp$xlevels$BsmtCond, levels(test.imp$BsmtCond))
lr.model.imp$xlevels$Heating <- union(lr.model.imp$xlevels$Heating, levels(test.imp$Heating))
lr.model.imp$xlevels$Electrical <- union(lr.model.imp$xlevels$Electrical, levels(test.imp$Electrical))
lr.model.imp$xlevels$MiscFeature <- union(lr.model.imp$xlevels$MiscFeature, levels(test.imp$MiscFeature))
lr.preds.imp <- predict(lr.model.imp,test.imp)
lr.RMSE.imp <- sqrt(sum((lr.preds.imp-test.imp$SalePrice)^2))/length(lr.preds.imp)
lr.RMSE.imp
qplot(lr.preds.imp,test.imp$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Imputed Linear Model Predictions",y="Imputed Sale Price ")

lr.model.onehot <- lm(SalePrice~.,train.onehot, na.action = na.omit)
lr.preds.onehot <- predict(lr.model.onehot,test.onehot)
lr.RMSE.onehot <- sqrt(sum((lr.preds.onehot-test.onehot$SalePrice)^2))/length(lr.preds.onehot)
lr.RMSE.onehot
qplot(lr.preds.onehot,test.onehot$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "One-hot Linear Model Predictions",y="One-hot Sale Price ")

summary(lr.model.onehot)
plot(lr.model.onehot$residuals)
significant_vars<-rownames(data.frame(summary(lr.model.onehot)$coef[summary(lr.model.onehot)$coef[,4] <= .01, 4]))
sig_coefficients<-lr.model.onehot$coefficients[significant_vars]

ggplot()+
  geom_bar(aes(x=reorder(names(sig_coefficients), -sig_coefficients),sig_coefficients),stat="identity")+
  theme_few()+
  labs(y="Linear Model Coefficients", x = "Variable")+
  coord_flip()
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p1<-ggplot(fullTrain, aes(x=RoofMatl, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p2<-ggplot(fullTrain, aes(x=Condition2, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p3<-ggplot(fullTrain, aes(x=GarageQual, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p4<-ggplot(fullTrain, aes(x=as.factor(BedroomAbvGr), y = SalePrice))+
  theme_few()+
  labs(x= "BedroomsAbvGr")+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)

fullTrain %>% group_by(RoofMatl) %>% summarize(n_houses = n(), avg_sale_price = mean(SalePrice), med_sale_price = median(SalePrice))


forward_subset<-regsubsets(SalePrice~.,data = one_hot_encoded_cc,method="forward")
forward<-names(one_hot_encoded_cc)[summary(forward_subset)$which[5,]]

backward_subset <-regsubsets(SalePrice~.,data = one_hot_encoded_cc,method="backward")
backward<-names(one_hot_encoded_cc)[summary(backward_subset)$which[5,]]

data.frame("Variable Importance Rank"=1:6,
           "Forward Selected Variables" = names(one_hot_encoded_cc)[summary(forward_subset)$which[5,]], 
           "Backward Selected Variables" = names(one_hot_encoded_cc)[summary(backward_subset)$which[5,]])
```
Below we applied the Bagging regression of Salesprice: nbagg =25(bootstrap replications i.e samples to replicate.)Bagging regression also known as  (bootstrap aggregating) was introduced by Breiman (1 996a) to reduce the variance of a predictor( Buhlmann & Yu,2002) In our case applying the Bagging regression on the predictor sale price will reduce its variance and improve its mean squared error. after applying the bagging regression, the Out-of-bag estimate of root mean squared error is 36260.96, the mean is 180827and there have been 25 samples which were replicated
=======
```{r}
fullTrain.bagging<-bagging(SalePrice~.,data=fullTrain,coob=TRUE,na.action = na.omit)
print(fullTrain.bagging)
summary(fullTrain.bagging)
fullTrain_SalePrice <- predict(fullTrain.bagging, 
                                         fullTrain)
summary(fullTrain_SalePrice)


```


# KNN regression
```{r}
train.onehot<- train.onehot[complete.cases(train.onehot),]
test.onehot<- test.onehot[complete.cases(test.onehot),]
fullTrain_knn3<-knnreg(train.onehot[-306],train.onehot$SalePrice, k=3);
summary(fullTrain_knn3)
#plot(test.onehot$SalePrice,predict(fullTrain_knn3,test.onehot[-306]))
(predict(fullTrain_knn3,test.onehot[-306]))
#using k =5
fullTrain_knn5<-knnreg(train.onehot[-306],train.onehot$SalePrice, k=5);
summary(fullTrain_knn5)
summary(test.onehot$SalePrice,predict(fullTrain_knn5,test.onehot[-306]))
sd(test.onehot$SalePrice,predict(fullTrain_knn5,test.onehot[-306]))
print(predict(fullTrain_knn5,test.onehot[-306]))
plot(test.onehot$SalePrice,predict(fullTrain_knn5,test.onehot[-306]))
#using k =19
fullTrain_knn19<-knnreg(train.onehot[-306],train.onehot$SalePrice, k=19);
summary(test.onehot$SalePrice,predict(fullTrain_knn19,test.onehot[-306]))
sd(test.onehot$SalePrice,predict(fullTrain_knn19,test.onehot[-306]))
plot(test.onehot$SalePrice,predict(fullTrain_knn15,test.onehot[-306]))

```

we applied  Hierarchical clustering in order to classify our data into 5 clustersin hierarchy.This was to answer our second research questions "Are there intrinsic groupings/clusters of houses present in the data and what do these clusters represent?
"we classified the clusters in 5 groups. The 5 groups represent a range of sale prices in which the dataset fall. The 4th group being the most expensive costing within the range between 300000(3e+05) to 400000(4e+05) with outliers that go beyond 600000(6e+05) while the cheapest group  is the 5th group ranging from 100000(1e+05) but less than 200000(2e+05). Moreover we checked the overall quality compared to the price and the 4th group having the highest quality ranging within 7 up to 10 while the 5th group had the lowest quality ranging within 2 up to 6 with a few outliers which have 7. Moreover, we checked as well the relationship betweenGrLivArea(Above grade (ground) living area square feet) and the sales price , we found that the 4th group's Gridliv area ranged between 1500 to 3500 with some outliers reaching to 4000 while the 5th group ranged between 500 to 2500.Interestingly,the second group's Gridliv area ranges between 1000 till almost 2000 however there are outliers that go beyond 5000.
```{r}
par(mar=rep(1, 4))
d <- dist(one_hot_encoded_cc, method = "euclidean") # distance matrix
hclust.fit <- hclust(d, method = "ward.D")
plot(hclust.fit) # display dendogram
hclust.groups <- cutree(hclust.fit, k = 5)
h_clust_data <- one_hot_encoded_cc %>% mutate(h_clust = hclust.groups)
h_clust_data$h_clust <- factor(h_clust_data$h_clust)
ggplot(h_clust_data,aes(h_clust,one_hot_encoded_cc$SalePrice,col=h_clust))+
  geom_boxplot()+
  theme_few()

ggplot(h_clust_data, aes(h_clust,OverallQual))+
  geom_boxplot()+
  theme_few()

ggplot(h_clust_data, aes(GrLivArea,SalePrice, col=h_clust))+
  geom_point()+
  theme_few()
ggplot(h_clust_data, aes(OverallQual,SalePrice,col=h_clust))+
  geom_point()+
  theme_few()

```
References
Peter Buhlmann & Bin Yu,2002,Analyzing Bagging