---
title: "Housing Data Analysis"
author: "Brooke Fitzgerald, Gloria Giramahoro, Sergi Drago"
date: "4/2/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gpairs)
library(ggthemes)
library(MASS)
library(dplyr)
library(Rtsne)
library(amap)
library(dummies)
library(rpart)
library(caret)
library(gam)
library(mice)
library(ipred)
library(kknn)
<<<<<<< HEAD
library(randomForest)
library(Metrics)
library(caret)

=======
=======
require(graphics)
>>>>>>> origin/master
library(rattle)
library(gridExtra)
library(leaps)
library(glmnet)
```

## Loading and cleanng the data

```{r importdata}
fullTrain <- read.csv("train.csv")
fullTest <- read.csv("test.csv")

fullTrain$LotFrontage[is.na(fullTrain$LotFrontage)]<-0.0

levels(fullTrain$Alley)[length(levels(fullTrain$Alley))+1] = "None"
fullTrain$Alley[is.na(fullTrain$Alley)]<-"None"

levels(fullTrain$PoolQC)[length(levels(fullTrain$PoolQC))+1] = "None"
fullTrain$PoolQC[is.na(fullTrain$PoolQC)]<-"None"

levels(fullTrain$Fence)[length(levels(fullTrain$Fence))+1] = "None"
fullTrain$Fence[is.na(fullTrain$Fence)]<-"None"

levels(fullTrain$MiscFeature)[length(levels(fullTrain$MiscFeature))+1] = "None"
fullTrain$MiscFeature[is.na(fullTrain$MiscFeature)]<-"None"

levels(fullTrain$GarageQual)[length(levels(fullTrain$GarageQual))+1] = "None"
fullTrain$GarageQual[is.na(fullTrain$GarageQual)]<-"None"

levels(fullTrain$GarageCond)[length(levels(fullTrain$GarageCond))+1] = "None"
fullTrain$GarageCond[is.na(fullTrain$GarageCond)]<-"None"

levels(fullTrain$BsmtQual)[length(levels(fullTrain$BsmtQual))+1] = "None"
fullTrain$BsmtQual[is.na(fullTrain$BsmtQual)]<-"None"

levels(fullTrain$BsmtCond)[length(levels(fullTrain$BsmtCond))+1] = "None"
fullTrain$BsmtCond[is.na(fullTrain$BsmtCond)]<-"None"

levels(fullTrain$BsmtExposure)[length(levels(fullTrain$BsmtExposure))+1] = "None"
fullTrain$BsmtExposure[is.na(fullTrain$BsmtExposure)]<-"None"

levels(fullTrain$BsmtFinType1)[length(levels(fullTrain$BsmtFinType1))+1] = "None"
fullTrain$BsmtFinType1[is.na(fullTrain$BsmtFinType1)]<-"None"

levels(fullTrain$BsmtFinType2)[length(levels(fullTrain$BsmtFinType2))+1] = "None"
fullTrain$BsmtFinType2[is.na(fullTrain$BsmtFinType2)]<-"None"

levels(fullTrain$FireplaceQu)[length(levels(fullTrain$FireplaceQu))+1] = "None"
fullTrain$FireplaceQu[is.na(fullTrain$FireplaceQu)]<-"None"

levels(fullTrain$GarageYrBlt)[length(levels(fullTrain$GarageYrBlt))+1] = "None"
fullTrain$GarageYrBlt[is.na(fullTrain$GarageYrBlt)]<-"None"

levels(fullTrain$GarageType)[length(levels(fullTrain$GarageType))+1] = "None"
fullTrain$GarageType[is.na(fullTrain$GarageType)]<-"None"

levels(fullTrain$GarageFinish)[length(levels(fullTrain$GarageFinish))+1] = "None"
fullTrain$GarageFinish[is.na(fullTrain$GarageFinish)]<-"None"

dim(fullTrain[complete.cases(fullTrain),])
```
```{r,echo=FALSE}
fullTest$LotFrontage[is.na(fullTest$LotFrontage)]<-0.0

levels(fullTest$Alley)[length(levels(fullTest$Alley))+1] = "None"
fullTest$Alley[is.na(fullTest$Alley)]<-"None"

levels(fullTest$PoolQC)[length(levels(fullTest$PoolQC))+1] = "None"
fullTest$PoolQC[is.na(fullTest$PoolQC)]<-"None"

levels(fullTest$Fence)[length(levels(fullTest$Fence))+1] = "None"
fullTest$Fence[is.na(fullTest$Fence)]<-"None"

levels(fullTest$MiscFeature)[length(levels(fullTest$MiscFeature))+1] = "None"
fullTest$MiscFeature[is.na(fullTest$MiscFeature)]<-"None"

levels(fullTest$GarageQual)[length(levels(fullTest$GarageQual))+1] = "None"
fullTest$GarageQual[is.na(fullTest$GarageQual)]<-"None"

levels(fullTest$GarageCond)[length(levels(fullTest$GarageCond))+1] = "None"
fullTest$GarageCond[is.na(fullTest$GarageCond)]<-"None"

levels(fullTest$BsmtQual)[length(levels(fullTest$BsmtQual))+1] = "None"
fullTest$BsmtQual[is.na(fullTest$BsmtQual)]<-"None"

levels(fullTest$BsmtCond)[length(levels(fullTest$BsmtCond))+1] = "None"
fullTest$BsmtCond[is.na(fullTest$BsmtCond)]<-"None"

levels(fullTest$BsmtExposure)[length(levels(fullTest$BsmtExposure))+1] = "None"
fullTest$BsmtExposure[is.na(fullTest$BsmtExposure)]<-"None"

levels(fullTest$BsmtFinType1)[length(levels(fullTest$BsmtFinType1))+1] = "None"
fullTest$BsmtFinType1[is.na(fullTest$BsmtFinType1)]<-"None"

levels(fullTest$BsmtFinType2)[length(levels(fullTest$BsmtFinType2))+1] = "None"
fullTest$BsmtFinType2[is.na(fullTest$BsmtFinType2)]<-"None"

levels(fullTest$FireplaceQu)[length(levels(fullTest$FireplaceQu))+1] = "None"
fullTest$FireplaceQu[is.na(fullTest$FireplaceQu)]<-"None"

levels(fullTest$GarageType)[length(levels(fullTest$GarageType))+1] = "None"
fullTest$GarageType[is.na(fullTest$GarageType)]<-"None"

levels(fullTest$GarageFinish)[length(levels(fullTest$GarageFinish))+1] = "None"
fullTest$GarageFinish[is.na(fullTest$GarageFinish)]<-"None"

fullTest<-dplyr::select(fullTest,-GarageYrBlt)
fullTrain<-dplyr::select(fullTrain,-GarageYrBlt)
```

## Exploratory Data Analysis

```{r}
summary(fullTrain)

ggplot(fullTrain, aes(x=Neighborhood,y=SalePrice))+
       geom_boxplot()+
       theme_few()+
       theme(axis.text.x = element_text(angle = 90, hjust = 1))
options(scipen=5)
hist(fullTrain$SalePrice, xlab = "Sale Price", main = "Histogram of Sale Price")

ggplot(fullTrain, aes(x=YearBuilt,y=SalePrice))+
       geom_point(alpha = 0.4)+
       theme_few()+
       theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(fullTrain, aes(x=OverallQual,y=SalePrice))+
       geom_jitter(alpha = 0.5)+
       theme_few()
ggplot(fullTrain, aes(x=TotRmsAbvGrd,y=SalePrice, col = OverallQual))+
       geom_jitter(alpha = 0.7)+
       theme_few()
fullTrain %>% group_by(MoSold) %>% summarize(NumSold=n(),MeanSalePrice=mean(SalePrice))%>%
ggplot(aes(x=MoSold,y=NumSold, fill=MeanSalePrice))+
       geom_bar(stat='identity')+
       scale_x_continuous(breaks = 1:12)+
       theme_few()
fullTrain %>% group_by(MoSold) %>% summarize(NumSold=n(),MedianSalePrice=median(SalePrice))%>%
ggplot(aes(x=MoSold,y=NumSold, fill=MedianSalePrice))+
       geom_bar(stat='identity')+
       scale_x_continuous(breaks = 1:12)+
       theme_few()

```

To do a preliminary examination of the data, we wanted to do some dimensionality reduction to see if there were any obvious structures in the data.

First, we computed PCA on the data with two different algorithms, then plotted them. 
```{r}
numeric_columns_only <- fullTrain %>% select_if(is.numeric)
numeric_full_columns_only <- numeric_columns_only[complete.cases(numeric_columns_only),]

p<-pca(as.matrix(numeric_full_columns_only))

x<-princomp(as.matrix(numeric_full_columns_only))

numeric_col_w_prcmp <- numeric_full_columns_only %>% mutate(princomp1 = x$scores[,1], princomp2 = x$scores[,2], pca1 = p$scores[,1], pca2 = p$scores[,2])

cols_w_prcmp <- inner_join(numeric_col_w_prcmp, fullTrain)

outlier <- cols_w_prcmp %>% filter(princomp2<(-10000))

cols_w_prcmp <- cols_w_prcmp %>% filter(princomp2>(-10000))

ggplot(cols_w_prcmp, aes(x=pca1, y=pca2, col=MSSubClass))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
ggplot(cols_w_prcmp, aes(x=princomp1, y=princomp2, col=MSSubClass))+
       geom_point(shape=21,alpha = .9)+
       theme_few()
```

Then we decided to compare dimensionality reduction of a t-SNE plot, which stands for t-distributed Stochastic Neighbor Embedding. It is a dimensionality reduction algorithm that embeds high dimensional data into a 2 dimensional space. There is a nice [R vignette about the package](https://cran.r-project.org/web/packages/tsne/tsne.pdf), a [blog post](https://www.codeproject.com/Tips/788739/Visualization-of-High-Dimensional-Data-using-t-SNE) about its use in R, and an [interactive visualization](http://distill.pub/2016/misread-tsne/) characterizing its strengths and weaknesses.

```{r}
housing_tsne<-Rtsne(numeric_full_columns_only)

numeric_col_w_tsne<- numeric_full_columns_only %>% mutate(tsne1 = housing_tsne$Y[,1],tsne2 = housing_tsne$Y[,2])

cols_w_tsne <- inner_join(numeric_col_w_tsne, fullTrain)

ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=log(SalePrice)))+
       geom_point()+
       theme_few()
ggplot(cols_w_tsne, aes(x=tsne1,y=tsne2,col=MSSubClass))+
       geom_point()+
       theme_few()
```

Next we wanted to do some one hot variable encoding in order to test correlation between SalePrice and factor variables.
```{r}
one_hot_encoded<- dummy.data.frame(fullTrain)
one_hot_encoded_cc <- one_hot_encoded[complete.cases(one_hot_encoded),]

corprice<-cor(one_hot_encoded_cc)[,"SalePrice"]
corprice<-corprice[names(corprice)!="SalePrice"]
sub_corprice <- corprice[abs(corprice)>0.5]

ggplot()+
  geom_bar(aes(x=reorder(names(sub_corprice), -sub_corprice),sub_corprice),stat="identity")+
  theme_few()+
  coord_flip()+
  labs(y="Correlation to SalePrice", x = "Variable")

ggplot(one_hot_encoded_cc, aes(x=SalePrice,y=GrLivArea))+
  geom_point()+
  theme_few()
```


```{r}
housing_tsne<-Rtsne(one_hot_encoded_cc)

numeric_col_w_tsne<- one_hot_encoded_cc %>% mutate(tsne1 = housing_tsne$Y[,1],tsne2 = housing_tsne$Y[,2])

ggplot(numeric_col_w_tsne, aes(x=tsne1,y=tsne2,col=log(GrLivArea)))+
       geom_point()+
       theme_few()
```

## Removing NA values
Because some predictive models don't allow for NA values and because every single observation has at least one NA value, we want to do some data imputation. The data was imputed using random forests. This imputed data can be used in conjunction with the one-hot encoded data to compare and contrast the various predictions.

```{r, echo=F}
imp <- mice(fullTrain,method='rf')
imputed <- fullTrain

# This function takes in a row of categories, then returns the category occuring most often
f<- function(dataRow){
       t<-table(dataRow)
       return(names(sort(t,decreasing = TRUE)[1]))
}
i=1
for(varName in imp$imp){
       if (!is.null(varName[[1]])){
              if (is.numeric(varName[[1]])){
                    # if data is numeric, impute the mean 
                     imputed[row.names(varName),names(imputed)[i]] = round(rowMeans(varName))
              } else {
                    # if data is categorical, impute the most often predicted category
                     imputed[row.names(varName),names(imputed)[i]] = apply(varName, 1, f)
              }
              
       }
       i<-i+1
}

```
## Starting Predictive Modeling
The first step in predictive modeling is splitting the data into training and test sets.

```{r}
indices<- createDataPartition(fullTrain$SalePrice,p=0.75)$Resample1
train.full<- fullTrain[indices,]
test.full<-fullTrain[-indices,]

train.imp<- imputed[indices,]
test.imp<-imputed[-indices,]

train.onehot<- one_hot_encoded[indices,]
test.onehot<-one_hot_encoded[-indices,]

train.onehot.cc <-train.onehot[complete.cases(train.onehot),]
test.onehot.cc <-test.onehot[complete.cases(test.onehot),]
```

The first model we tried out was a decision tree.

```{r}
tree.model.full <-rpart(SalePrice~.,train.full)
tree.preds.full <- predict(tree.model.full, test.full)
tree.RMSE.full <- sqrt(sum((tree.preds.full-test.full$SalePrice)^2))/length(tree.preds.full)
tree.RMSE.full
p1<-qplot(tree.preds.full,test.full$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Full Regression Tree Predictions",y="Sale Price ")

tree.model.onehot <-rpart(SalePrice~.,train.onehot)
tree.preds.onehot <- predict(tree.model.onehot, test.onehot)
tree.RMSE.onehot <- sqrt(sum((tree.preds.onehot-test.onehot$SalePrice)^2))/length(tree.preds.onehot)
tree.RMSE.onehot
p2<-qplot(tree.preds.onehot,test.onehot$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "One-hot Regression Tree Predictions",y="Sale Price ")

tree.model.imp <- rpart(SalePrice~.,train.imp)
tree.preds.imp <- predict(tree.model.imp, test.imp)
tree.RMSE.imp <- sqrt(sum((tree.preds.imp-test.imp$SalePrice)^2))/length(tree.preds.imp)
tree.RMSE.imp
p3<-qplot(tree.preds.imp,test.imp$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Imputed Regression Tree Predictions",y="Sale Price ")

tree.preds.avg <- rowMeans(cbind(tree.preds.full,tree.preds.onehot,tree.preds.imp))
tree.RMSE.avg <- sqrt(sum((tree.preds.avg-test.full$SalePrice)^2))/length(tree.preds.avg)
tree.RMSE.avg
p4<-qplot(tree.preds.avg,test.full$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Average Regression Tree Predictions",y="Sale Price ")

grid.arrange(p1,p2,p3,p4,ncol=2,nrow=2)

fancyRpartPlot(tree.model.full)
fancyRpartPlot(tree.model.onehot)
fancyRpartPlot(tree.model.imp)

lr.model.full <- lm(SalePrice~.,train.full, na.action = na.omit)
lr.model.full$xlevels$Condition2 <- union(lr.model.full$xlevels$Condition2, levels(test.full$Condition2))
lr.model.full$xlevels$RoofMatl <- union(lr.model.full$xlevels$RoofMatl, levels(test.full$RoofMatl))
lr.model.full$xlevels$BsmtCond <- union(lr.model.full$xlevels$BsmtCond, levels(test.full$BsmtCond))
lr.model.full$xlevels$Heating <- union(lr.model.full$xlevels$Heating, levels(test.full$Heating))
lr.model.full$xlevels$Electrical <- union(lr.model.full$xlevels$Electrical, levels(test.full$Electrical))
lr.model.full$xlevels$MiscFeature <- union(lr.model.full$xlevels$MiscFeature, levels(test.full$MiscFeature))
lr.model.full$xlevels$Exterior1st <- union(lr.model.full$xlevels$Exterior1st, levels(test.full$Exterior1st))
lr.model.full$xlevels$Exterior2nd <- union(lr.model.full$xlevels$Exterior2nd, levels(test.full$Exterior2nd))
lr.model.full$xlevels$Functional <- union(lr.model.full$xlevels$Functional, levels(test.full$Functional))

lr.preds.full <- predict(lr.model.full,test.full)
lr.RMSE.full <- sqrt(sum((lr.preds.full-test.full$SalePrice)^2))/length(lr.preds.full)
lr.RMSE.full
qplot(lr.preds.full,test.full$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Full Linear Model Predictions",y="Full Sale Price ")

lr.model.imp <- lm(SalePrice~.,train.imp)
lr.model.imp$xlevels$Condition2 <- union(lr.model.imp$xlevels$Condition2, levels(test.imp$Condition2))
lr.model.imp$xlevels$Exterior1st <- union(lr.model.imp$xlevels$Exterior1st, levels(test.imp$Exterior1st))
lr.model.imp$xlevels$Exterior2nd <- union(lr.model.imp$xlevels$Exterior2nd, levels(test.imp$Exterior2nd))
lr.model.imp$xlevels$RoofMatl <- union(lr.model.imp$xlevels$RoofMatl, levels(test.imp$RoofMatl))
lr.model.imp$xlevels$Functional <- union(lr.model.imp$xlevels$Functional, levels(test.imp$Functional))
lr.model.imp$xlevels$BsmtCond <- union(lr.model.imp$xlevels$BsmtCond, levels(test.imp$BsmtCond))
lr.model.imp$xlevels$Heating <- union(lr.model.imp$xlevels$Heating, levels(test.imp$Heating))
lr.model.imp$xlevels$Electrical <- union(lr.model.imp$xlevels$Electrical, levels(test.imp$Electrical))
lr.model.imp$xlevels$MiscFeature <- union(lr.model.imp$xlevels$MiscFeature, levels(test.imp$MiscFeature))
lr.preds.imp <- predict(lr.model.imp,test.imp)
lr.RMSE.imp <- sqrt(sum((lr.preds.imp-test.imp$SalePrice)^2))/length(lr.preds.imp)
lr.RMSE.imp
qplot(lr.preds.imp,test.imp$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "Imputed Linear Model Predictions",y="Imputed Sale Price ")

lr.model.onehot <- lm(SalePrice~.,train.onehot, na.action = na.omit)
lr.preds.onehot <- predict(lr.model.onehot,test.onehot)
lr.RMSE.onehot <- sqrt(sum((lr.preds.onehot-test.onehot$SalePrice)^2))/length(lr.preds.onehot)
lr.RMSE.onehot
qplot(lr.preds.onehot,test.onehot$SalePrice)+theme_few()+geom_abline(slope=1,intercept=0)+labs(x = "One-hot Linear Model Predictions",y="One-hot Sale Price ")

summary(lr.model.onehot)
plot(lr.model.onehot$residuals)
significant_vars<-rownames(data.frame(summary(lr.model.onehot)$coef[summary(lr.model.onehot)$coef[,4] <= .01, 4]))
sig_coefficients<-lr.model.onehot$coefficients[significant_vars]

ggplot()+
  geom_bar(aes(x=reorder(names(sig_coefficients), -sig_coefficients),sig_coefficients),stat="identity")+
  theme_few()+
  labs(y="Linear Model Coefficients", x = "Variable")+
  coord_flip()
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p1<-ggplot(fullTrain, aes(x=RoofMatl, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p2<-ggplot(fullTrain, aes(x=Condition2, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p3<-ggplot(fullTrain, aes(x=GarageQual, y = SalePrice))+
  theme_few()+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p4<-ggplot(fullTrain, aes(x=as.factor(BedroomAbvGr), y = SalePrice))+
  theme_few()+
  labs(x= "BedroomsAbvGr")+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(p1,p2,p3,p4,nrow=2,ncol=2)

fullTrain %>% group_by(RoofMatl) %>% summarize(n_houses = n(), avg_sale_price = mean(SalePrice), med_sale_price = median(SalePrice))


forward_subset<-regsubsets(SalePrice~.,data = one_hot_encoded_cc,method="forward")
forward<-names(one_hot_encoded_cc)[summary(forward_subset)$which[5,]]

backward_subset <-regsubsets(SalePrice~.,data = one_hot_encoded_cc,method="backward")
backward<-names(one_hot_encoded_cc)[summary(backward_subset)$which[5,]]

data.frame("Variable Importance Rank"=1:6,
           "Forward Selected Variables" = names(one_hot_encoded_cc)[summary(forward_subset)$which[5,]], 
           "Backward Selected Variables" = names(one_hot_encoded_cc)[summary(backward_subset)$which[5,]])
```
# bagging regressionof Salesprice: nbagg =25(bbotstrap replications)
=======

##The kNN Algorithm
```{r}
fullTrain$Id <- NULL
fullTrain.bagging<-bagging(SalePrice~.,data=fullTrain,coob=TRUE,na.action = na.omit)
print(fullTrain.bagging)
summary(fullTrain.bagging)
fullTrain_SalePrice <- predict(fullTrain.bagging, 
                                         fullTrain)
summary(fullTrain_SalePrice)
 

```


# KNN regression
```{r}
train.onehot<- train.onehot[complete.cases(train.onehot),]
test.onehot<- test.onehot[complete.cases(test.onehot),]
fullTrain_knn<- knnreg(train.onehot[-306],train.onehot$SalePrice, k=3);
plot(test.onehot$SalePrice,predict(fullTrain_knn,test.onehot[-306]))
summary


```
In order to test the significance of variables
```{r}
lm_1 <- lm(train.onehot$SalePrice ~ , data = train.onehot )
summary(lm_1)
summary(lm_1)$fstatistic
# The significance measure we want:
summary(lm_1)$fstatistic[1]
#We need a vector to hold the outputs to avoid working with them one at a time.
exp_var_fstat <- as.numeric(rep(NA, times = 30))
names(exp_var_fstat) <- names(train.onehot)
#Now run through the variables, get the linear fit and store the F-statistic
```

## Hierarchical clustering
```{r}
par(mar=rep(1, 4))
d <- dist(one_hot_encoded_cc, method = "euclidean") # distance matrix
hclust.fit <- hclust(d, method = "ward.D")
plot(hclust.fit) # display dendogram
hclust.groups <- cutree(hclust.fit, k = 5)
h_clust_data <- one_hot_encoded_cc %>% mutate(h_clust = hclust.groups)
h_clust_data$h_clust <- factor(h_clust_data$h_clust)
ggplot(h_clust_data, aes(h_clust,SalePrice))+
  geom_boxplot()+
  theme_few()

ggplot(h_clust_data, aes(h_clust,OverallQual))+
  geom_boxplot()+
  theme_few()

ggplot(h_clust_data, aes(GrLivArea,SalePrice, col=h_clust))+
  geom_point()+
  theme_few()
```
```{r}
hc <- hclust(dist(one_hot_encoded_cc),"ave")
plot(hc)
plot(hc, hang = -1)

## Do the same with centroid clustering and *squared* Euclidean distance,
## cut the tree into ten clusters and reconstruct the upper part of the
## tree from the cluster centers.
hc <- hclust(dist(one_hot_encoded_cc)^2, "cen")
memb <- cutree(hc, k = 3)
cent <- NULL
for (k in 1:3) {
  cent <- rbind(cent, colMeans(one_hot_encoded_cc[memb == k,]))
}
hc1 <- hclust(dist(cent)^2, method = "cen", members = table(memb))
opar <- par(mfrow = c(1, 2))
plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
par(opar)



```
<<<<<<< HEAD
<<<<<<< HEAD
=======
#model <- train.kknn(SalePrice ~ ., data = fullTrain, kmax = 1459,data.na= False)
##plot(fullTrain$SalePrice~fullTrain$SaleType)
## We want to classify the sold houses in 3 main groups 
# installing/loading the latest installr package:
install.packages("installr"); library(installr) # install+load installr
 updateR() # updating R.
 library(ggplots2)
 ## bagging classification trees & regression trees
 install.packages("rpart")
 library(rpart)
 install.packages("ipred")
library(resample)
library(caret)
library(mlbench)
library(caretEnsemble)
 fullTrain$Id <- NULL
 fullTrain.subset<-fullTrain[,10:81]
 na.omit(fullTrain.subset)
 summary(fullTrain.subset)
 control <- trainControl(method="repeatedcv", number=10, repeats=3)
 seed <- 7
 metric <- "Accuracy"
 set.seed(seed)
 fit.treebag <- train(SalePrice~., data=fullTrain.subset, method="treebag", metric=metric, trControl=control,na.rm=TRUE )
 set.seed(seed)
 fit.rf <- train(SalePrice~., data=fullTrain.subset, method="rf",metric=metric, trControl=control,na.rm=TRUE)
 bagging_results <- resamples(list(treebag=fit.treebag, rf=fit.rf))
 summary(bagging_results)
 fullTrain.bagging<-bagging(SalePrice~.,data=fullTrain.subset,coob=TRUE) 
summary(fullTrain.bagging)
fullTrain.subsetpred.class<-predict(fullTrain.bagging,fullTrain.subset$SalePrice)
##clustering
#1. K-means
#Determine the number of clusters
# scaling the data by dividing each variable by its standard deviation:
std <- apply(fullTrain[,-1], 2, sd) # finding standard deviations of variables
fullTrain_SD <- sweep(fullTrain[,-1],2,std,FUN="/") 
# Calculating pairwise Euclidean distances between the standardized variables:
distance_fullTrain <- dist(fullTrain_SD)
# plotting a Single network:
fullTrain_SingleNet<- hclust(distance_fullTrain, method='single')
plclust(fullTrain_SingleNet, labels=factors, ylab="Distance")
# plotting a complete network:
fullTrain_completeNet<-hclust(distance_fullTrain,method='complete')
plclust(fullTrain_completeNet, labels=factors, ylab="Distance")
# plotting an average network:
fullTrain_avgNet <- hclust(distance_fullTrain, method='average')
plclust(fullTrain_avgNet, labels=factors, ylab="Distance")
# Note the complete linkage algorithm is slightly less prone to forming 
# "outlier-only" clusters here.
# Cutting the complete-linkage dendrogram to form k=2 clusters here:
cut_2 <- cutree(fullTrain_completeNet, k=2)
cut_2     # printing the "clustering vector"
fullTrain_cluster2 <- lapply(1:2, function(nc) fullTrain[cut_2==nc]) summary(fullTrain_cluster2) # printing the clusters in terms of the factors labels
# Suppose we preferred a 5-cluster solution:
cut_5 <- cutree(fullTrain_completeNet, k=5)
# Equivalently, in this case:
cut_5 <- cutree(fullTrain_completeNet, h=2)  
# h specifies the height at which the dendrogram should be cut
cut_5   # printing the "clustering vector"
fullTrain_cluster5 <- lapply(1:5, function(nc) fullTrain[cut_5==nc])  
summary(fullTrain_cluster5)   # printing the clusters in terms of the Food labels


############# Visualization of Clusters:
### Via the scatterplot matrix:

pairs(fullTrain[,-1], panel=function(x,y) text(x,y,cut_5))

# Cluster 1 seems to be the high-fat, high-energy foods (beef, ham, pork)
# Cluster 2 foods seem to have low iron (more white meats than red meats)
# Cluster 4 foods have low protein (the clams)
# Cluster 5 is a high-calcium outlier (canned sardines)

### Via a plot of the scores on the first 2 principal components, 
### with the clusters separated by color:

fullTrain.pc <- princomp(fullTrain[,-1],cor=T)

# Setting up the colors for the 5 clusters on the plot:
my.color.vector <- rep("green", times=nrow(food))
my.color.vector[cut.5==2] <- "red"
my.color.vector[cut.5==3] <- "orange"
my.color.vector[cut.5==4] <- "green"
my.color.vector[cut.5==5] <- "yellow"

# Plotting the PC scores:

par(pty="s")
plot(fullTrain.pc$scores[,1], food.pc$scores[,2], ylim=range(food.pc$scores[,1]), 
     xlab="PC 1", ylab="PC 2", type ='n', lwd=2)
text(food.pc$scores[,1], food.pc$scores[,2], labels=Food, cex=0.7, lwd=2,
     col=my.color.vector)
>>>>>>> origin/master



#Random Forests



#first we define multiplot function which will be used afterwards
```{r}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  plots <- c(list(...), plotlist)
  numPlots = length(plots)
  if (is.null(layout)) {
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  if (numPlots==1) {
    print(plots[[1]])
  } else {
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    for (i in 1:numPlots) {
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```    

#i still need to change it for the actual data
```{r}
train <- read.csv("cleanedData.csv")
train

str(train)

#View sale price since there are many NA values we only need to keep rows 1 - 1460
train$SalePrice

#training predictors
trainX = train[1:1460,3:81]
#training targets
trainY = train[1:1460,82]
#Combine training predictors and targets
trainXY = cbind(trainX,"SalePrice" = trainY)
#"GarageYrBlt" has 104 levels...way too many for a random forest model so 
# we fix that by treating "GarageYrBlt" as a continuous predictor
str(trainXY)
trainXY$GarageYrBlt = as.numeric(trainXY$GarageYrBlt)
trainX$GarageYrBlt = as.numeric(trainX$GarageYrBlt)
```


```{r}
#5-fold cross validation
set.seed(100)
folds = createFolds(trainXY$SalePrice,k = 5)
train1 = trainXY[-folds[[1]],1:80]
train2 = trainXY[-folds[[2]],1:80]
train3 = trainXY[-folds[[3]],1:80]
train4 = trainXY[-folds[[4]],1:80]
train5 = trainXY[-folds[[5]],1:80]

#cross validation "test" set (including the target value "SalePrice")
cvTest1 = trainXY[folds[[1]],1:80]
cvTest2 = trainXY[folds[[2]],1:80]
cvTest3 = trainXY[folds[[3]],1:80]
cvTest4 = trainXY[folds[[4]],1:80]
cvTest5 = trainXY[folds[[5]],1:80]

#Build 5 models
modelRF1 = randomForest(SalePrice ~ ., data = train1)
modelRF2 = randomForest(SalePrice ~ ., data = train2)
modelRF3 = randomForest(SalePrice ~ ., data = train3)
modelRF4 = randomForest(SalePrice ~ ., data = train4)
modelRF5 = randomForest(SalePrice ~ ., data = train5)

#Make predictions using cross validation "test" sets
Pred1 = predict(modelRF1, cvTest1)
Pred2 = predict(modelRF2, cvTest2)
Pred3 = predict(modelRF3, cvTest3)
Pred4 = predict(modelRF4, cvTest4)
Pred5 = predict(modelRF5, cvTest5)

#Plot target vs. predicted for the 5 models
tp1 = ggplot(data = as.data.frame(cbind("Target" = cvTest1$SalePrice,"Predicted" = Pred1)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + ggtitle("Target vs. Predicted")
tp2 = ggplot(data = as.data.frame(cbind("Target" = cvTest2$SalePrice,"Predicted" = Pred2)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + ggtitle("Target vs. Predicted")
tp3 = ggplot(data = as.data.frame(cbind("Target" = cvTest3$SalePrice,"Predicted" = Pred3)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + ggtitle("Target vs. Predicted")
tp4 = ggplot(data = as.data.frame(cbind("Target" = cvTest4$SalePrice,"Predicted" = Pred4)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + ggtitle("Target vs. Predicted")
tp5 = ggplot(data = as.data.frame(cbind("Target" = cvTest5$SalePrice,"Predicted" = Pred5)), aes(x = Target, y = Predicted)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlim(0,5e05) + ylim(0, 5e05) + ggtitle("Target vs. Predicted")
multiplot(tp1,tp2,tp3,tp4,tp5,cols = 2)

```

```{r}
#Calculate residuals
Res1 = cvTest1$SalePrice - Pred1
Res2 = cvTest2$SalePrice - Pred2
Res3 = cvTest3$SalePrice - Pred3
Res4 = cvTest4$SalePrice - Pred4
Res5 = cvTest5$SalePrice - Pred5


#Calculate RMSE
rmse1 = rmse(cvTest1$SalePrice,Pred1)
rmse2 = rmse(cvTest2$SalePrice,Pred2)
rmse3 = rmse(cvTest3$SalePrice,Pred3)
rmse4 = rmse(cvTest4$SalePrice,Pred4)
rmse5 = rmse(cvTest5$SalePrice,Pred5)
cbind(rmse1,rmse2,rmse3,rmse4,rmse5)

#feature importance
Imp1 = as.data.frame(modelRF1$importance)
Imp1$Predictor = rownames(Imp1)
Imp1 = Imp1[,c(2,1)]
rownames(Imp1) = c(1:nrow(Imp1))
colnames(Imp1)[2] = "Importance"
ImpOrd1 = Imp1[order(Imp1$Importance, decreasing = TRUE),]

Imp2 = as.data.frame(modelRF2$importance)
Imp2$Predictor = rownames(Imp2)
Imp2 = Imp2[,c(2,1)]
rownames(Imp2) = c(1:nrow(Imp2))
colnames(Imp2)[2] = "Importance"
ImpOrd2 = Imp2[order(Imp2$Importance, decreasing = TRUE),]

Imp3 = as.data.frame(modelRF3$importance)
Imp3$Predictor = rownames(Imp3)
Imp3 = Imp3[,c(2,1)]
rownames(Imp3) = c(1:nrow(Imp3))
colnames(Imp3)[2] = "Importance"
ImpOrd3 = Imp3[order(Imp3$Importance, decreasing = TRUE),]

Imp4 = as.data.frame(modelRF4$importance)
Imp4$Predictor = rownames(Imp4)
Imp4 = Imp4[,c(2,1)]
rownames(Imp4) = c(1:nrow(Imp4))
colnames(Imp4)[2] = "Importance"
ImpOrd4 = Imp4[order(Imp4$Importance, decreasing = TRUE),]

Imp5 = as.data.frame(modelRF5$importance)
Imp5$Predictor = rownames(Imp5)
Imp5 = Imp5[,c(2,1)]
rownames(Imp5) = c(1:nrow(Imp5))
colnames(Imp5)[2] = "Importance"
ImpOrd5 = Imp5[order(Imp5$Importance, decreasing = TRUE),]

cbind(ImpOrd1,ImpOrd2,ImpOrd3,ImpOrd4,ImpOrd5)
```

=======
>>>>>>> origin/master
